---
title: "ë”°ë¦‰ì´ ë°ì´í„° ë¶„ì„í•˜ê¸° (4) Transformation"
tags:
- Project
- PCA
- Transformation
- Python
category: Project
use_math: true
header: 
 teaser: /assets/img/ë”°ë¦‰ì´_transform_1.png
---
{% raw %}
## ë”°ë¦‰ì´ ë°ì´í„° ë¶„ì„í•˜ê¸° (4) Transformation

ì´ë²ˆì—ëŠ” PCAë¥¼ ë¹„ë¡¯í•´ ì˜ˆì¸¡ë³€ìˆ˜ì˜ ë°ì´í„°ì…‹ì„ ë³€í™˜ì‹œí‚¤ëŠ”<sup>transformation</sup> ì—¬ëŸ¬ ê°€ì§€ ë°©ë²•ë“¤ì— ëŒ€í•´ ë‹¤ë£¨ì–´ë³´ë„ë¡ í•˜ê² ë‹¤. ëŒ€í‘œì ìœ¼ë¡œ PCAëŠ” ê¸°ë³¸ì ì¸ íšŒê·€ë¬¸ì œì— ì‘ìš©ë˜ì–´ PCRë¡œ ì‚¬ìš©ë˜ê±°ë‚˜, ê³ ì°¨ì› ë¬¸ì œì˜ ì°¨ì› ì¶•ì†Œ ê¸°ë²•ìœ¼ë¡œ í•„ìˆ˜ì ì¸ ì—­í• ì„ í•œë‹¤. ì—¬ê¸°ì„œëŠ” ìš°ì„  PCAë¥¼ ì§„í–‰í•˜ê³ , ì´ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ PCRì„ ì§„í–‰í•˜ì—¬ ì´ë¥¼ PLSì™€ ë¹„êµí•´ë³´ë„ë¡ í•˜ì.

### PCA

PCAëŠ” scikit-learnì˜ `sklearn.decomposition.PCA`ë¥¼ ì´ìš©í•˜ë„ë¡ í•˜ê² ë‹¤.  ë‹¤ë§Œ, data transformationì—ì„œëŠ” dataì˜ scaleì´ ì¤‘ìš”í•˜ê¸° ë•Œë¬¸ì— `StandardScaler()`ì„ ì´ìš©í•´ ë°ì´í„°ë¥¼ í‘œì¤€í™”ì‹œí‚¨ í›„ PCAë¥¼ ì§„í–‰í•˜ë„ë¡ í•˜ê² ë‹¤. ì´ë¥¼ ìœ„í•´ ì´ë²ˆì—ëŠ” `Pipeline`ì´ë¼ëŠ” `scikit-learn`ì˜ íˆ´ì„ ì´ìš©í•´ ì¼ë ¨ì˜ ì „ì²˜ë¦¬ê³¼ì •ë¶€í„° PCAê¹Œì§€ë¥¼ ë„ì‹í™”í•˜ê³  ê°€ì‹œì ìœ¼ë¡œ ìœ ìš©í•œ ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ë„ë¡ í•˜ê² ë‹¤(`Pipeline`ì€ scikit-learnì˜ ê½ƒì´ë‹ˆ ë°˜ë“œì‹œ ì‚¬ìš© í…Œí¬ë‹‰ì„ ìµíˆë„ë¡ í•˜ìğŸ˜ƒ). 

ìš°ì„  í•„ìš”í•œ ëª¨ë“ˆë“¤ì„ ë‹¤ìŒê³¼ ê°™ì´ ë¡œë“œí•˜ë„ë¡ í•˜ì.

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
import statsmodels.api as sm

np.random.seed(37)
```

Data loadì˜ ê³¼ì •ì€ ì´ì „ì— í–ˆë˜ ê²ƒê³¼ ìœ ì‚¬í•˜ì§€ë§Œ, NA ê°’ì„ ë²„ë¦¬ì§€ ì•Šê³  ì¶”í›„ Imputerë¥¼ ì´ìš©í•´ ê²°ì¸¡ê°’ì„ median<sup>ì¤‘ê°„ê°’</sup>ìœ¼ë¡œ ì²˜ë¦¬í•  ê²ƒì´ë¯€ë¡œ, `dropna()`ë¥¼ ë°°ì œí–ˆìœ¼ë©°, train_test split ì´ì „ì— ìš°ì„  ì˜ˆì¸¡ë³€ìˆ˜ì™€ ë°˜ì‘ë³€ìˆ˜ë¥¼ ê°ê° ë°ì´í„°í”„ë ˆì„, ë²¡í„°ë¡œ ë¶„ë¦¬í–ˆë‹¤. ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤,

```python
# Data Load
df_train = pd.read_csv('train.csv')
df_train = df_train.iloc[:,1:]
df_train.columns = ['hour', 'temp', 'precip',
       'windspeed', 'humidity', 'visibility',
       'ozone', 'pm10', 'pm2_5', 'count']
df_X = df_train[df_train.columns.drop('count')]
df_y = df_train['count'].values
```

ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” `preprocessor`ë¥¼ ë§Œë“¤ê³ , ì´ë¥¼ PCA ëª¨ë“ˆê³¼ ì´ì–´ì§€ê²Œ í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•  ê²ƒì¸ë°, ì´ ê³¼ì •ì—ì„œ scikit-learnì˜ `ColumnTransformer()` ëª¨ë“ˆì„ ì´ìš©í•˜ë„ë¡ í•  ê²ƒì´ë‹¤. ì´ëŠ” íŠ¹ì • ì—´(feature)ë“¤ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ ì „ì²˜ë¦¬ ê³¼ì •ì„ ì ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ”ë°, í˜„ì¬ ë‹¤ë£¨ê³ ì í•˜ëŠ” ë°ì´í„°ì—ì„œëŠ” precip ë³€ìˆ˜(`categorical_features`)ëŠ” 0/1ë¡œ ì½”ë”©ë˜ì–´ìˆìœ¼ë¯€ë¡œ `OneHotEncoder`ë¥¼ ì ìš©í•´ì•¼ í•˜ê³  ë‚˜ë¨¸ì§€ ë³€ìˆ˜ì— ëŒ€í•´ì„œëŠ” `StandardScaler()`ì„ ì ìš©í•´ì•¼ í•œë‹¤. ì´ë•Œ ê°œì¸ì ìœ¼ë¡œ ì‹œê°„ëŒ€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” hour ë³€ìˆ˜ë¥¼ ë³„ë„ë¡œ ì²˜ë¦¬í•´ì•¼í•œë‹¤ê³  ìƒê°í•´ `hour_feature`ë¡œ ë¶„ë¦¬í–ˆëŠ”ë°, `StandardScaler`ë¡œ ì²˜ë¦¬í•˜ëŠ” `preprocessor_1`ê³¼ hour ì „ì²´ë¥¼ One-hot encodingìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” `preprocessor_2`ë¥¼ ë§Œë“¤ì–´ ë¹„êµí•´ë³´ë„ë¡ í•  ê²ƒì´ë‹¤(*ì´ë•Œ, hourê°€ ì·¨í•˜ëŠ” ê°’ì´ 24ê°œì´ë¯€ë¡œ sparse matrixê°€ ìƒì„±ë˜ëŠ”ë°, ì´ ê²½ìš° PCAë°©ë²•ì´ ë‹¬ë¼ì ¸ì•¼ í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” sparse=Falseë¡œ ì„¤ì •í–ˆë‹¤*). ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

```python
# Data Preprocessing
numeric_features = list(df_X.columns.drop(['precip','hour']))
numeric_transformer = Pipeline(
    steps=[("imputer",SimpleImputer(strategy='median')),("scaler",StandardScaler())]
)
hour_feature = ['hour']
hour_transformer = Pipeline(
    steps=[("imputer",SimpleImputer(strategy='most_frequent')),('scaler',StandardScaler(with_std=False))]
 ) # hourì€ standardscalerì—ì„œ í‘œì¤€í¸ì°¨ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì€ ì œì™¸í•¨.
hour_transformer_oh = OneHotEncoder(handle_unknown='ignore',sparse=False) # hourì„ onehotencodingìœ¼ë¡œ ì²˜ë¦¬, sparse=FalseëŠ” sparse matrixë¡œ ë°˜í™˜í•˜ì§€ ì•Šê²Œ ì„¤ì •

categorical_features = ['precip']
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

preprocessor_1 = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ('hour', hour_transformer, hour_feature),
        ("cat", categorical_transformer, categorical_features)
    ]
)
preprocessor_2 = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ('hour', hour_transformer_oh, hour_feature),
        ("cat", categorical_transformer, categorical_features)
    ]
)
```

scikit-learnì—ëŠ” ì´ëŸ¬í•œ Pipeline ê°ì²´ë¥¼ html í˜•íƒœë¡œ ë„ì‹í™”í•´ì£¼ëŠ” `set_config`ë©”ì„œë“œê°€ ìˆë‹¤. ì•„ë˜ì™€ ê°™ì´ ì„¤ì •í•˜ë©´ ëœë‹¤.

```python
# Visualize Pipeline
from sklearn import set_config

set_config(display="diagram")
```

ì´í›„ PCAì™€ preprocessor_1,2 ë¥¼ ê°ê° í•©ì³ ê°ê°ì˜ ìƒˆë¡œìš´ Pipeline `pca_1`,`pca_2`ë¡œ ë§Œë“¤ì—ˆë‹¤. ì´í›„ Training data setì„ ì´ì „ì— ì‚¬ìš©í•œ `train_test_split` ë©”ì„œë“œë¥¼ ì´ìš©í•´ train dataì™€ validation dataë¡œ ë‚˜ëˆ„ì—ˆìœ¼ë©°, validation ë¹„ìœ¨ì€ ë§ˆì°¬ê°€ì§€ë¡œ 30%ë¥¼ ì ìš©í–ˆë‹¤. ê·¸ë¦¬ê³  train dataë¡œ ê°ê°ì˜ Pipelineì„ í•™ìŠµì‹œì¼°ë‹¤(`fit` method, ì•„ë˜ ì½”ë“œ ì°¸ê³ ).

```python
# Principal Component Regression by pipeline
pca_1 = Pipeline(
    steps=[("preprocessor",preprocessor_1),("pca",PCA())]
)
pca_2 = Pipeline(
    steps=[("preprocessor",preprocessor_2),("pca",PCA())]
)
X_train, X_val, y_train, y_val = train_test_split(df_X, df_y, test_size=0.3, random_state=0)
pca_1.fit(X_train,y_train)
pca_2.fit(X_train,y_train)
```

<img src="/assets/img/ë”°ë¦‰ì´_transform_0.png" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-04-12 á„‹á…©á„’á…® 3.27.26"/>

ê·¸ëŸ¬ë©´ ìœ„ì™€ ê°™ì´ Interactiveí•œ html ê°ì²´ê°€ ë‚˜ì˜¤ëŠ”ë°, ê° í•­ëª©ì„ í´ë¦­í•˜ë©´ ì ìš©ëœ ê° ë©”ì„œë“œì— ëŒ€í•´ hyperparmeterë‚˜ ì„¤ì •ì„ ì–´ë–»ê²Œ ì·¨í–ˆëŠ”ì§€ íŒŒì•…í•  ìˆ˜ ìˆë‹¤. ì´ì œ PCAê°€ ì ìš©ëœ ê²°ê³¼ë¥¼ íŒŒì•…í•´ë³´ë„ë¡ í•˜ì. Pipelineì˜ ê° ë‹¨ê³„ì™€ í•´ë‹¹ ë‹¨ê³„ì—ì„œì˜ attributeëŠ” ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥ë˜ê³  Pipelline í˜•ì„± ë‹¨ê³„ì—ì„œ ì„¤ì •í•œ ê° ë‹¨ê³„ì˜ ì´ë¦„ìœ¼ë¡œ ì´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ”ë°(`.named_steps['ì´ë¦„']` attribute ì´ìš©), ì•„ë˜ì™€ ê°™ì´ `pca_1`,`pca_2`ì—ì„œì˜` explained_variance_ratio`ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” ê° ì£¼ì„±ë¶„ì´ ì „ì²´ ë¶„ì‚°ì˜ ì–¼ë§ˆë§Œí¼ì˜ ë¹„ìœ¨ì„ ì„¤ëª…í•˜ëŠ”ì§€ ì˜ë¯¸í•œë‹¤. 

ë‹¤ìŒ ì½”ë“œë¥¼ í†µí•´ ìƒì„±í•œ ë°ì´í„°í”„ë ˆì„ `pca_res`ëŠ” ê° PCA pipeline ëª¨ë¸(Scaler : `StandardScaler`ë¥¼ ì ìš©í•œ `pca_1`, One-hot : `OneHotEncoder`ë¥¼ ì ìš©í•œ `pca_2`)ì˜ ì²«ë²ˆì§¸~10ë²ˆì§¸ ì£¼ì„±ë¶„ì´ ì„¤ëª…í•˜ëŠ” ì „ì²´ ë°ì´í„°ì˜ ë¶„ì‚°ë¹„ìœ¨ì„ ì˜ë¯¸í•œë‹¤(*ê²°ê³¼ëŠ” ì•„ë˜ í‘œ*).

```py
# PCA result
pca_step_1 = pca_1.named_steps['pca'] # load pca step for pca_1
pca_1_ratio = pca_step_1.explained_variance_ratio_.round(3)

pca_step_2 = pca_2.named_steps['pca']  # load pca step for pca_2
pca_2_ratio = pca_step_2.explained_variance_ratio_.round(3)

pca_res = pd.DataFrame([pca_1_ratio,pca_2_ratio], index=['Scaler','One-hot'], columns=range(1,11,1)).T
pca_res.iloc[:3,:].sum(axis=0) # Ex.ratio of first three components
```

\vert       \vert  Scaler \vert  One-hot \vert 
\vert  ---: \vert  -----: \vert  ------: \vert 
\vert     1 \vert   0.884 \vert    0.336 \vert 
\vert     2 \vert   0.039 \vert    0.232 \vert 
\vert     3 \vert   0.031 \vert    0.105 \vert 
\vert     4 \vert   0.015 \vert    0.080 \vert 
\vert     5 \vert   0.012 \vert    0.055 \vert 
\vert     6 \vert   0.008 \vert    0.049 \vert 
\vert     7 \vert   0.007 \vert    0.021 \vert 
\vert     8 \vert   0.003 \vert    0.008 \vert 
\vert     9 \vert   0.001 \vert    0.006 \vert 
\vert    10 \vert   0.000 \vert    0.006 \vert 

ì´ë¥¼ ë³´ë©´ One-hot encodingì„ ì²˜ë¦¬í•˜ì§€ ì•Šì€ ì²«ë²ˆì§¸ PCA ëª¨ë¸ì´ ë” íš¨ê³¼ì ìœ¼ë¡œ ì£¼ì„±ë¶„ ë¶„ë¦¬ê°€ ì¼ì–´ë‚¬ìŒì„ í™•ì¸í•  ìˆ˜ ìˆëŠ”ë°, `pca_res.iloc[:3,:].sum(axis=0)` ì½”ë“œë¡œ ì²˜ìŒ ì„¸ ê°œì˜ ì£¼ì„±ë¶„ì´ ì„¤ëª…í•˜ëŠ” ë¹„ìœ¨ì„ í™•ì¸í•´ë³´ë©´  ScalerëŠ” 95.4%, One-hotì€ 67.3% ì´ë‹¤. ë”°ë¼ì„œ, PCRê³¼ PLSë¥¼ ë¹„êµí•˜ëŠ” ê³¼ì •ì—ì„œëŠ” `preprocessor_1`ë§Œ ì´ìš©í•˜ê³ , PCA ë‹¨ê³„ì—ì„œëŠ” 3ê°œì˜ ì£¼ì„±ë¶„ì„ ì‚¬ìš©í•˜ë„ë¡ í•˜ê² ë‹¤.

### Principal Component Regression & Partial Least Squares

PCR<sup>ì£¼ì„±ë¶„íšŒê·€</sup>ì€ ì˜ˆì¸¡ë³€ìˆ˜í–‰ë ¬ì˜ ê³ ìœ ê°’ë¶„í•´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íšŒê·€ê³„ìˆ˜ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì´ë‹¤([ì°¸ê³ ](https://ddangchani.github.io/linear%20model/linearreg2/)). ì¦‰, PCAë¥¼ training dataì— ì ìš©ì‹œí‚´ìœ¼ë¡œì¨ ì°¨ì› ì¶•ì†Œê°€ ê°€ëŠ¥í•˜ê²Œ í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„ í˜• íšŒê·€ë¥¼ ì§„í–‰í•˜ëŠ” ê²ƒì´ë‹¤.  ì´ë•Œ PCAëŠ” ë°˜ì‘ë³€ìˆ˜ì— ë¬´ê´€í•˜ê²Œ ì‘ë™í•˜ë¯€ë¡œ, **unsupervised** transformationì´ ì¼ì–´ë‚œë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.[PLS](https://ddangchani.github.io/linear%20model/linearreg2/)ëŠ” Linear regression ì•Œê³ ë¦¬ì¦˜ì˜ ì¼ì¢…ì¸ë°, PCRê³¼ ìœ ì‚¬í•˜ê²Œ ì˜ˆì¸¡ë³€ìˆ˜ ì—´ë²¡í„°ë“¤ì˜ ì„ í˜•ê²°í•©ì„ ë°”íƒ•ìœ¼ë¡œ ì„ í˜• ëª¨í˜•ì„ êµ¬ì„±í•˜ì§€ë§Œ, ê·¸ ê³¼ì •ì—ì„œ ë°˜ì‘ë³€ìˆ˜ì™€ì˜ ê´€ê³„ê°€ ê°œì…ë˜ë¯€ë¡œ **supervised** transformationì´ë¼ëŠ” ê²ƒì´ PCRê³¼ì˜ ì°¨ì´ì ì´ë‹¤.

#### PCR

```python
# PCR
from sklearn.linear_model import LinearRegression
pcr = Pipeline(
    steps=[('Preprocessor',preprocessor_1),
    ('PCA',PCA(n_components=3)),
    ('Linear',LinearRegression())]
)
pcr.fit(X_train, y_train)
```

<img src="/assets/img/ë”°ë¦‰ì´_transform_1.png" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-04-12 á„‹á…©á„’á…® 3.53.18"/>

ìœ„ ì½”ë“œë¥¼ í†µí•´, ê·¸ë¦¼ê³¼ ê°™ì€ Pipelineì„ ê°–ëŠ” Principal Component Regression methodë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ, `sklearn.cross_decomposition`ì˜ `PLSRegression`ì„ ì´ìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ PLS íŒŒì´í”„ë¼ì¸ì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤.

#### PLS

```python
# PLS
from sklearn.cross_decomposition import PLSRegression
pls = Pipeline(
    steps=[("Preprocessor",preprocessor_1),
    ('PLS', PLSRegression(n_components=3))]
)
pls.fit(X_train, y_train)
```

<img src="/assets/img/ë”°ë¦‰ì´_transform_2.png" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-04-12 á„‹á…©á„’á…® 7.35.06"/>

#### Comparison between PCR / PLS

ì´ì œ PCRê³¼ PLSë¥¼ ë¹„êµí•´ë³´ì. ì„±ëŠ¥ ë¹„êµ ì´ì „ì—, ìš°ì„  validation dataë¥¼ ë°”íƒ•ìœ¼ë¡œ ë°ì´í„°ì…‹ì„ ì²«ë²ˆì§¸ ì£¼ì„±ë¶„ìœ¼ë¡œ ì •ì‚¬ì˜<sup>projection</sup>ì‹œì¼œ ë°˜ì‘ë³€ìˆ˜ì˜ ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì„ scatter plotìœ¼ë¡œ í™•ì¸í•´ë³´ë„ë¡ í•˜ì. ì´ë•Œ ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ì„ íƒí•œ ì´ìœ ëŠ” ì•ì„œ PCAì—ì„œ 95.4% ì„¤ëª…ë ¥ì„ ê°–ëŠ”ë‹¤ëŠ” ê²ƒì„ í™•ì¸í–ˆê¸° ë•Œë¬¸ì— ê°€ëŠ¥í•˜ë‹¤. í˜¹ì‹œ ë‘ ë²ˆì§¸ ì„±ë¶„ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•˜ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì½”ë“œë¥¼ ì‰½ê²Œ ìˆ˜ì •í•˜ë©´ ë  ê²ƒì´ë‹¤. ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

```python
# PCR vs. PLS plot
pca = pcr[0:2] # Preprocess to PCA
fig, axes = plt.subplots(1, 2, figsize=(10,3))
# PCA vs PCR
axes[0].scatter(
    pca.transform(X_val)[:,0], y_val, alpha = 0.3, label = 'True' # alpha as transparaency
) # 1st principal component vs true y
axes[0].scatter(
    pca.transform(X_val)[:,0], pcr.predict(X_val), alpha = 0.3,  label = 'Pred'
) # 1st principal component vs pred y
axes[0].set(
    xlabel="Projected X_val onto first PCA component", ylabel='y', title = 'PCR / PCA'
)
axes[0].legend()
# PLS
axes[1].scatter(
    pls.transform(X_val)[:,0], y_val, alpha = 0.3, label = 'True'
)
axes[1].scatter(
    pls.transform(X_val)[:,0], pls.predict(X_val), alpha = 0.3, label = 'Pred'
)
axes[1].set(
    xlabel="Projected X_val onto first PCA component", ylabel='y', title = 'PLS'
)
axes[1].legend()
plt.tight_layout()
plt.savefig('plots/pcr_vs_pls.png', transparent=False, facecolor = 'white')
```

ì²« ì¤„ì˜ `pca`ëŠ” PCR íŒŒì´í”„ë¼ì¸ì—ì„œ Linear Regressionì„ ì œì™¸í•œ PCAê¹Œì§€ì˜ í”„ë¡œì„¸ìŠ¤ë§Œ ë¶„ë¦¬í•œ ë˜ë‹¤ë¥¸ íŒŒì´í”„ë¼ì¸ì´ë‹¤. ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë‘ plotì„ ì–»ì„ ìˆ˜ ìˆëŠ”ë°, ì™¼ìª½ì€ PCAë¥¼ í†µí•´ ì–»ì€ ë¶„í¬ì™€ PCRì„ í†µí•´ ì–»ì€ ì˜ˆì¸¡ê°’ì˜ ë¶„í¬ë¥¼ ë³´ì´ë©°, ì˜¤ë¥¸ìª½ì€ PLSë¥¼ í†µí•´ ì–»ì€ ë¶„í¬ì™€ ì˜ˆì¸¡ê°’ì˜ ë¶„í¬ë¥¼ ë‚˜íƒ€ë‚¸ ê²ƒì´ë‹¤.

![pcr_vs_pls](/assets/img/ë”°ë¦‰ì´_transform_3.png){: .align-center}

ê·¸ë¦¼ìœ¼ë¡œë§Œ ë³´ë©´ ì„±ëŠ¥ì´ ì‰½ê²Œ êµ¬ë¶„ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ, validation dataì— ëŒ€í•œ R-squared valueì™€ RMSE valueë¥¼ ëª¨ë‘ ë¹„êµí•´ë³´ë„ë¡ í•˜ì. ê° pipelineì˜ ê²½ìš° ëª¨ë‘ Regression model ì´ê³ , `.score` ë©”ì„œë“œë¡œ  ê²°ì •ê³„ìˆ˜ $R^2$ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

```python
from sklearn.metrics import mean_squared_error
# R_squared
print(pcr.score(X_val, y_val).round(3)) # PCR = 0.544
print(pls.score(X_val, y_val).round(3)) # PLS = 0.617

# RMSE
print(np.sqrt(mean_squared_error(y_val, pcr.predict(X_val))).round(3)) # PCR = 54.958
print(np.sqrt(mean_squared_error(y_val, pls.predict(X_val))).round(3)) # PLS = 50.374
```

ë¹„êµ ê²°ê³¼, $R^2$ ì¸¡ë©´ì—ì„œëŠ” PCRì´ ìš°ìˆ˜í•œ ê²ƒìœ¼ë¡œ íŒŒì•…ë˜ì—ˆë‹¤. ë°˜ë©´ RMSE ì¸¡ë©´ì—ì„œëŠ” PLSê°€ ë” ìš°ìˆ˜í•œ ê°’ì„ ê°€ì§€ëŠ” ê²ƒìœ¼ë¡œ íŒŒì•…ë˜ì—ˆëŠ”ë°, ì´ëŠ” ì´ì „ê¹Œì§€ shrinkage method, model selectionì„ í†µí•´ ì–»ì€ RMSEê°’ë“¤ë³´ë‹¤ ë”ìš± ìš°ìˆ˜í•œ ê°’ì´ë‹¤ (ì˜ë¯¸ìˆëŠ” ì§„ì „ğŸ˜ƒ).

# References

- scikit-learn ê³µì‹ ë¬¸ì„œ : https://scikit-learn.org/stable/auto_examples/cross_decomposition/plot_pcr_vs_pls.html

{% endraw %}