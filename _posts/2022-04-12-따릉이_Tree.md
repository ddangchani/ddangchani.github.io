---
title: "ë”°ë¦‰ì´ ë°ì´í„° ë¶„ì„í•˜ê¸° (5) Tree"
tags:
- Project
- Tree
- Random Forest
- Python
category: Project
use_math: true
header: 
 teaser: /assets/img/ë”°ë¦‰ì´_Tree_5.png
---

{% raw %}
## ë”°ë¦‰ì´ ë°ì´í„° ë¶„ì„í•˜ê¸° (5) Tree

ì´ë²ˆì—ëŠ” Tree ê´€ë ¨ ëª¨ë¸ë“¤ë¡œ ì£¼ì–´ì§„ ë°ì´í„°ì…‹ì„ í›ˆë ¨ì‹œì¼œë³´ê³  ì´ë¥¼ ê²€ì¦í•´ë³´ë„ë¡ í•˜ì. ì €ë²ˆ Transformation ë°ì´í„° ë¶„ì„ ê³¼ì •ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ `scikit-learn`ì˜ `Pipeline`ì„ ì´ìš©í•´ ë°ì´í„° ì „ì²˜ë¦¬ë¶€í„° ëª¨ë¸ë§ê¹Œì§€ì˜ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•´ë³´ë„ë¡ í•˜ê² ë‹¤. Data Loadì™€ Preprocessing ê´€ë ¨ ì½”ë“œ ë° ìì„¸í•œ ì„¤ëª…ì€ ì‹œë¦¬ì¦ˆì˜ ì´ì „ ê²Œì‹œê¸€ì„ ì°¸ê³ í•˜ë©´ ë  ê²ƒì´ë‹¤.

### Decision Tree Regressor

ìš°ì„  ë‹¤ìŒ êµ¬ì¡°ì™€ ê°™ì€ ê°€ì¥ ê°„ë‹¨í•œ í˜•íƒœì˜ Tree Regressorì„ ë§Œë“¤ì–´ë³´ë„ë¡ í•˜ì.

<img src="/assets/img/ë”°ë¦‰ì´_Tree_0.png" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-04-21 á„‹á…©á„’á…® 2.12.13"/>

ì´ì „ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ Pipeline ëª¨ë“ˆì„ ì´ìš©í•´ ëª¨ë¸ì„ êµ¬ì„±í–ˆë‹¤. ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

```py
# Tree Regressor
from sklearn.tree import DecisionTreeRegressor
tree_reg = Pipeline(
    steps=[("preprocessor",preprocessor),
    ("tree",DecisionTreeRegressor(criterion="squared_error"))]
)
tree_reg.fit(X_train, y_train)
```

ì—¬ê¸°ì„œëŠ” Tree Regression ê³¼ì •ì˜ criterionìœ¼ë¡œ ì œê³±ì˜¤ì°¨ë¥¼ ì§€ì •í•œ ê²ƒ ì™¸ì—ëŠ” ë³„ë‹¤ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì •í•˜ì§€ ì•Šì•˜ë‹¤. ë”°ë¼ì„œ ì–´ë–¤ í˜•íƒœë¡œ ëª¨ë¸ì´ í˜•ì„±ë˜ì—ˆëŠ”ì§€ ì‚´í´ë³´ë©´, ì•„ë˜ì™€ ê°™ì´ treeì˜ ê¹Šì´(depth)ê°€ 20ì´ê³  ì´ leave(*íŠ¸ë¦¬ì˜ ì ë…¸ë“œ : ìì‹ ë…¸ë“œê°€ ì—†ëŠ” ë…¸ë“œë¥¼ ì˜ë¯¸í•¨*)ì˜ ê°œìˆ˜ëŠ” 967ê°œì¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë˜í•œ, ìœ„ ëª¨ë¸ë¡œ Validation dataì— ëŒ€í•´ RMSEë¥¼ ê³„ì‚°í•´ ë³´ë©´ 50.643ì˜ ê°’ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.

```py
# Tree Info and Validation RMSE
tree_res = tree_reg.named_steps['tree']
tree_res.get_depth() # 20
tree_res.get_n_leaves() # 967
tree_pred = tree_reg.predict(X_val)
np.sqrt(mean_squared_error(y_val, tree_pred)).round(3) # 50.643
```

#### GridSearch

í•˜ì§€ë§Œ, ê°œë³„ íŠ¸ë¦¬ë¥¼ ë§Œë“œëŠ” ë°ì—ë„ ì•ì„œ ì–¸ê¸‰í•œ íŠ¸ë¦¬ì˜ ê¹Šì´, ì ë…¸ë“œì˜ ê°œìˆ˜ ë“±ì„ í¬í•¨í•œ ë§ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ìš”êµ¬ëœë‹¤. ì ë‹¹í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ê¸° ìœ„í•´ GridSearchë¥¼ ì´ìš©í•´ ì°¾ì•„ë³´ë„ë¡ í•˜ì. Scikit-learn íŒ¨í‚¤ì§€ëŠ” `GridSearchCV`ë¼ëŠ” cross-validationê³¼ Grid serachë¥¼ ë™ì‹œì— ì§„í–‰í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ê°–ê³  ìˆìœ¼ë‚˜, ì—¬ê¸°ì„œëŠ” ì´ë¯¸ ì£¼ì–´ì§„ validation ë°ì´í„°ë¥¼ í™œìš©í•˜ê¸° ìœ„í•´ GridSearchë§Œ ì§„í–‰í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ì§ì ‘ ì½”ë”©í–ˆë‹¤. ì½”ë“œëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

```py
# Check whether Overfitted with Cross-Validation
from tqdm import * # Progress-bar
from sklearn.model_selection import GridSearchCV
params = {'max_features':("sqrt","log2","auto"), "max_depth" : list(range(2, 41, 2))}

# GridSearch Algorithm
N_comb = len(params['max_features']) * len(params['max_depth'])
with tqdm(total=N_comb) as pbar:
    df = []
    for feat in params['max_features']:
        scores = []
        for d in params['max_depth']:
            reg_i = Pipeline(
                steps=[('preprocessor',preprocessor),('tree',DecisionTreeRegressor(max_features=feat,max_depth=d))]
            )
            reg_i.fit(X_train,y_train)
            pred_i = reg_i.predict(X_val)
            score_i = np.sqrt(mean_squared_error(y_val, pred_i)).round(3)
            scores.append(score_i)
            pbar.update()
        df.append(scores)
res = pd.DataFrame(df, index=params['max_features'], columns=params['max_depth']).T
```

GridSearchëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° `max_features`ì™€ `max_depth`ì— ëŒ€í•´ì„œë§Œ ì§„í–‰í–ˆë‹¤. ì´ëŠ” ê°ê° split ë‹¨ê³„ì—ì„œ ëª‡ ê°œì˜ featureì„ ëœë¤ìœ¼ë¡œ ì¶”ì¶œí•´ best splitting variableì„ ì°¾ì„ ê²ƒì¸ì§€ì™€, íŠ¸ë¦¬ì˜ ê¹Šì´ë¥¼ ìµœëŒ€ ì–´ëŠì •ë„ë¡œ ì„¤ì •í•  ê²ƒì¸ì§€ë¥¼ ì˜ë¯¸í•œë‹¤. ìœ„ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ê²°ê³¼ë¡œ ë°ì´í„°í”„ë ˆì„ `res`ë¥¼ ì–»ëŠ”ë°, ì´ëŠ” ê° hyperparmeter ì¡°í•©(ì´ $3\times20=60$)ì— ëŒ€í•œ Validation RMSE ê°’ì„ í¬í•¨í•œë‹¤. ì´ë¥¼ Plotìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤(ì½”ë“œëŠ” Github ì°¸ê³ ).

<img src="/assets/img/ë”°ë¦‰ì´_Tree_1.png" alt="GridSearch_Tree"/>

ì´ë¥¼ í™•ì¸í•´ë³´ë©´ ëª¨ë‘ `max_depth`ê°€ 4,6,8,10 ë“± ë‚®ì€ ê°’ì—ì„œ Validation RMSEê°€ ë‚®ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë©°, `max_feature`ì˜ ê²½ìš° `auto`(ê° ë‹¨ê³„ì—ì„œ *ëª¨ë“ * ë³€ìˆ˜ë¥¼ ê³ ë ¤í•¨)ê°€ ë‹¤ë¥¸ ë‘ ê¸°ì¤€ë³´ë‹¤(`sqrt` : ê° ë‹¨ê³„ì—ì„œ $\sqrt p$ ë§Œí¼ ê³ ë ¤, `log2` : $\log_2 p$ ë§Œí¼ ê³ ë ¤) ë” ë‚˜ì€ ì§€í‘œë¥¼ ë³´ì´ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

#### Tree with PCA

ì´ë²ˆì—ëŠ” íŠ¸ë¦¬ ëª¨ë¸ ì´ì „ì— PCAë¥¼ ì ìš©ì‹œì¼œ ì°¨ì› ì¶•ì†Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë” ë‚˜ì€ ì„±ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•´ë³´ë„ë¡ í•˜ì. ì½”ë“œëŠ” ì•„ë˜ì™€ ê°™ì€ë°, ìš°ì„  ê° ì¸ë±ìŠ¤ `idx`ëŠ” ê° `max_features`ì— ëŒ€í•´ ê°€ì¥ ë‚®ì€ RMSEë¥¼ ê°–ëŠ”(ìµœì ì˜) `max_depth` íŒŒë¼ë¯¸í„° ê°’ì„ ì˜ë¯¸í•œë‹¤. ì¦‰ `idx_1`ì˜ ê²½ìš° `"auto"` ëª¨ë¸ì„ ìµœì†Œë¡œ í•˜ëŠ” `max_depth=6`ì˜ ê°’ì´ ë  ê²ƒì´ë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ PCAë¥¼ í¬í•¨í•œ Pipelineì„ ë§Œë“¤ì—ˆëŠ”ë°, ì—¬ê¸°ì„œ `n_components=3`ì„ ì„ íƒí•œ ì´ìœ ëŠ” ì´ì „ Transformation ê¸€ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê° ëª¨ë¸ì´ ë°ì´í„°ë“¤ì„ ì–´ë–»ê²Œ fittingí•˜ëŠ”ì§€ ì‚´í´ë³´ë„ë¡ í•˜ì. ì´ë¥¼ ìœ„í•´ Validation ë°ì´í„°ë“¤ì„ ì²«ë²ˆì§¸ ì£¼ì„±ë¶„(principal component)ìœ¼ë¡œ projectioní•œ í›„, ì´ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•´ predict valueì— ëŒ€í•œ Line plotì„ ê·¸ë ¤ë³´ë„ë¡ í•˜ì. ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

```py
# Three Models : For each max_feature, the argmin max_depth
idx_1 = res.index[np.argmin(res['auto'])]
idx_2 = res.index[np.argmin(res['log2'])]
idx_3 = res.index[np.argmin(res['sqrt'])]


tree_pca_1 = Pipeline(
    steps=[('preprocessor',preprocessor),
        ('pca', PCA(n_components=3)),
        ('tree',DecisionTreeRegressor(max_features='auto',max_depth=idx_1))]
)
tree_pca_2 = Pipeline(
    steps=[('preprocessor',preprocessor),
        ('pca', PCA(n_components=3)),
        ('tree',DecisionTreeRegressor(max_features='log2',max_depth=idx_2))]
)
tree_pca_3 = Pipeline(
    steps=[('preprocessor',preprocessor),
        ('pca', PCA(n_components=3)),
        ('tree',DecisionTreeRegressor(max_features='sqrt',max_depth=idx_3))]
)
tree_pca_1.fit(X_train, y_train)
tree_pca_2.fit(X_train, y_train)
tree_pca_3.fit(X_train, y_train)

pca = tree_pca_1[0:2] # Pipeline until PCA
xs_1, ys_1 = zip(*sorted(zip(pca.transform(X_val)[:,0],tree_pca_1.predict(X_val))))
xs_2, ys_2 = zip(*sorted(zip(pca.transform(X_val)[:,0],tree_pca_2.predict(X_val))))
xs_3, ys_3 = zip(*sorted(zip(pca.transform(X_val)[:,0],tree_pca_3.predict(X_val))))
```

```py
# Compare three Local_Minimum RMSE spot
# Projeted on the first Principal Component

# Plot
fig, axes = plt.subplots(3,1,figsize=(10,15), constrained_layout=True)
axes[0].scatter(pca.transform(X_val)[:,0], y_val, alpha=0.4, label='data')
axes[0].plot(xs_1, ys_1, label = 'max_feature = "auto"', color = "darkgreen", linewidth = 2)
axes[0].legend(fontsize = 12, loc = 'upper right')
axes[0].set(xlabel = "First Principal Component", ylabel="Y",title=f"Tree with PCA : max_depth = {idx_1}")

axes[1].scatter(pca.transform(X_val)[:,0], y_val, alpha=0.4, label='data')
axes[1].plot(xs_2, ys_2, label = 'max_feature = "log2"', color = "orange", linewidth = 2)
axes[1].legend(fontsize = 12, loc = 'upper right')
axes[1].set(xlabel = "First Principal Component", ylabel="Y",title=f"Tree with PCA : max_depth = {idx_2}")

axes[2].scatter(pca.transform(X_val)[:,0], y_val, alpha=0.4, label='data')
axes[2].plot(xs_3, ys_3, label = 'max_feature = "sqrt"', color = "navy", linewidth = 2)
axes[2].legend(fontsize = 12, loc = 'upper right')
axes[2].set(xlabel = "First Principal Component", ylabel="Y",title=f"Tree with PCA : max_depth = {idx_3}")


plt.savefig("plots/tree_pca.png", transparent=False, facecolor='white')
```

ìœ„ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì„¸ ê°œì˜ plotì„ ì–»ì„ ìˆ˜ ìˆë‹¤.

<img src="/assets/img/ë”°ë¦‰ì´_Tree_2.png" alt="tree_pca"/>

ì´ë¥¼ ì‚´í´ë³´ë©´ `max_depth`ê°€ ê°™ì§€ë§Œ `max_features`ê°€ ë‹¤ë¥¸ `auto`,`log2` ëª¨ë¸ì— ëŒ€í•´ì„œëŠ” plotì˜ ì í•©ë„ê°€ ìœ ì‚¬í•˜ì§€ë§Œ, `max_depth=6`ìœ¼ë¡œ íŠ¸ë¦¬ì˜ ê¹Šì´ê°€ ë” ê¹Šì€ `sqrt` ëª¨ë¸ì— ëŒ€í•´ì„œëŠ” ë” outlyingí•œ ë°ì´í„°ì— ëŒ€í•´ì„œë„ ì˜ˆì¸¡ì´ ì˜ ì´ë£¨ì–´ì§€ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. `max_features`ë¥¼ ë™ì¼í•˜ê²Œ ë‘ê³ (`auto`) íŠ¸ë¦¬ì˜ ê¹Šì´ì— ë”°ë¥¸ ì˜í–¥ì„ í™•ì¸í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì€ë°(ì½”ë“œëŠ” Github ì°¸ê³ ), `max_depth`ê°€ 20ì¸ ê²½ìš°ì™€ 36ì¸ ê²½ìš° RMSEê°€ ìœ ì‚¬í•œ ê²ƒê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ëª¨ë¸ ì—­ì‹œ, ë” outlyingí•œ ë°ì´í„°ë¥¼ 36ì¼ ë•Œ ì˜ ìºì¹˜í•œë‹¤ëŠ” ê²ƒ ì™¸ì—ëŠ” ì „ì²´ì ìœ¼ë¡œ ìœ ì‚¬í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

![tree_pca_by_depth](/assets/img/ë”°ë¦‰ì´_Tree_3.png){: .align-center}

### Gradient Boosted Decision Trees

ì´ë²ˆì—ëŠ” [GBM](https://ddangchani.github.io/Gradient-Boosting-Machine)ì„ ì´ìš©í•œ Decision Tree Regression ëª¨ë¸(Gradient Boosted Decision Trees, ì´í•˜ GBDT)ì„ ì‚´í´ë³´ë„ë¡ í•˜ì. scikit-learnì˜ `sklearn.ensemble` ëª¨ë“ˆ ì—ì„œëŠ” Regressionì— ì‚¬ìš©ê°€ëŠ¥í•œ GBDT ëª¨ë¸ `GradientBoostingRegressor` ì„ ì œê³µí•˜ê³  ìˆë‹¤. ì´ë¥¼ ì´ìš©í•˜ì—¬ ì´ë²ˆì—ëŠ” scikit-learnì˜ ìì²´ gridsearch moduleì¸ `sklearn.model_selection.GridSearchCV`ìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ì§„í–‰í•´ë³´ë„ë¡ í•˜ê² ë‹¤. ì´ë•Œ, `GridSearchCV`ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ Validation scoringì„ ìœ„í•œ ê¸°ì¤€ì´ í•„ìš”í•œë°, ì—¬ê¸°ì„œëŠ” RMSEë¥¼ ì‚¬ìš©í•  ê²ƒì´ë¯€ë¡œ ì´ë¥¼ ë³„ë„ë¡œ ì •ì˜í•´ì£¼ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤. ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

```py
# GBDT with GridsearchCV
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer

# RMSE def for Gridsearch Scoring
def rmse(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    return(np.sqrt(mse))
rmse_score = make_scorer(rmse, greater_is_better=False)
# Pipeline Model
tree_gbm = Pipeline([
("preprocessor",preprocessor),("gbm",GradientBoostingRegressor(loss='squared_error',max_features='auto'))
])
```

ìœ„ì™€ ê°™ì´ scoring ê°ì²´ë¥¼ ë§Œë“¤ê³  GBDTì˜ íŒŒì´í”„ë¼ì¸ì„ ë§Œë“¤ë©´, ì•„ë˜ì™€ ê°™ì´ parameter gridë¥¼ ì •ì˜í•˜ì—¬ GridSearchë¥¼ ì§„í–‰í•  ìˆ˜ ìˆë‹¤.

```py
# Gridsearch
param_grid = {
    'gbm__n_estimators' : 10 * np.logspace(base=2, start=0, stop=5, num=6).astype(int),
    'gbm__max_depth' : np.linspace(4,32,8).astype(int)
}
search = GridSearchCV(
    tree_gbm, param_grid, verbose = 5, n_jobs= 5, scoring = rmse_score
)
search.fit(X_train, y_train)
```

ì´ì „ê³¼ëŠ” ë‹¤ë¥´ê²Œ, gridsearch parameterë¡œ `max_feature` ëŒ€ì‹  `n_estimators`ë¥¼ ì´ìš©í–ˆë‹¤. ì´ëŠ” Boosting ë‹¨ê³„ê°€ ëª‡ë²ˆ ë°˜ë³µë ì§€ë¥¼ ì •ì˜í•˜ëŠ” ê²ƒìœ¼ë¡œ, ì¼ë°˜ì ìœ¼ë¡œ Gradient boosting ë°©ì‹ì€ ê³¼ì í•©ì— **robust**í•˜ë¯€ë¡œ í° ê°’ì´ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ê°€ì§€ëŠ” ê²½í–¥ì´ ìˆë‹¤. fittingì„ ì§„í–‰í•˜ê²Œ ë˜ë©´, `search.cv_results_`ë¡œ íŒŒë¼ë¯¸í„° ì¡°í•© ë³„(ì´ 240ê°œ) fit/score time(í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ì‹œê°„), ê° splitë³„ score(ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ì¸ 5-folds validation ì´ë¯€ë¡œ ì´ 5ê°œ ê°’)ë“¤ê³¼ í‰ê· , í‘œì¤€í¸ì°¨ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì–»ì–´ ì´ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤(ì•„ë˜ í‘œ ì°¸ê³ ). 

| mean_fit_time | std_fit_time | mean_score_time | std_score_time | param_gbm__max_depth | param_gbm__n_estimators |                                         params | split0_test_score | split1_test_score | split2_test_score | split3_test_score | split4_test_score | mean_test_score | std_test_score | rank_test_score |
| ------------: | -----------: | --------------: | -------------: | -------------------: | ----------------------: | ---------------------------------------------: | ----------------: | ----------------: | ----------------: | ----------------: | ----------------: | --------------: | -------------: | --------------: |
|         0.249 |        0.063 |           0.061 |          0.016 |                    4 |                      10 | {'gbm__max_depth': 4, 'gbm__n_estimators': 10} |           -53.869 |           -45.526 |           -53.144 |           -51.671 |           -48.405 |         -50.523 |          3.125 |              17 |

âœ…*ì°¸ê³ ë¡œ, GridSearch ë°©ì‹ì€ ì¼ë°˜ì ì¸ ë…¸íŠ¸ë¶ í™˜ê²½ì—ì„œëŠ” í›ˆë ¨ ì‹œê°„ì´ ìƒë‹¹íˆ ë§ì´ ì†Œìš”ë˜ê¸° ë•Œë¬¸ì— ë‚˜ì˜ ê²½ìš° **Google Colab**ì„ ì´ìš©í–ˆê³ , Colabì—ì„œ ì–»ì€ `search.cv_results_` ë°ì´í„°í”„ë ˆì„ì„ ë³„ë„ì˜ ë¡œê·¸ë¡œ ì €ì¥í•´ ë‹¤ì‹œ ë¡œì»¬ì—ì„œ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ë²•ì„ ì´ìš©í–ˆë‹¤.*

#### Plot for GBDT Gridsearch

ì´ë ‡ê²Œ ì–»ì€ GridSearch ê²°ê³¼ë¥¼ ì´ìš©í•´ íŒŒë¼ë¯¸í„° ì¡°í•© ë³„ Mean test scoreì„ ì•„ë˜ì™€ ê°™ì´ plotìœ¼ë¡œ ë‚˜íƒ€ë‚´ë³´ë„ë¡ í•˜ì. *(ì½”ë“œ ì°¸ê³  : https://stackoverflow.com/questions/37161563/how-to-graph-grid-scores-from-gridsearchcv  - ì˜¤ë¥˜ê°€ ìˆì–´ ì¼ë¶€ ì½”ë“œ ìˆ˜ì •í•¨)*

```py
# Plot search_res
def plot_grid_search(cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2, greater_is_better):
    # Get Test Scores Mean and std for each grid search
    if greater_is_better: 
        scores_mean = cv_results['mean_test_score']
    else: # greater_is_better = Falseë©´ score ê°’ì´ ìŒìˆ˜ì„
        scores_mean = -cv_results['mean_test_score']

    scores_mean = np.array(scores_mean).reshape(len(grid_param_1),len(grid_param_2))

    # Plot Grid search scores
    fig, ax = plt.subplots(1,1, figsize = (10,10))

    # Param1 is the X-axis, Param 2 is represented as a different curve (color line)
    for idx, val in enumerate(grid_param_2):
        ax.plot(grid_param_1, scores_mean[:,idx], '-o', label= name_param_2 + ': ' + str(val))

    ax.set_title("Grid Search Scores", fontsize=15, fontweight='bold')
    ax.set_xlabel(name_param_1, fontsize=12)
    ax.set_ylabel('CV Average Score', fontsize=12)
    ax.legend(loc="best", fontsize=12)
    ax.grid('on')
    plt.savefig('plots/tree_gbm_gridsearchcv.png', facecolor='white', transparent=False)

plot_grid_search(search_res, 
    param_grid['gbm__max_depth'],
    param_grid['gbm__n_estimators'],
    'max_depth', 'n_estimators', greater_is_better=False)
```

![tree_gbm_gridsearchcv](/assets/img/ë”°ë¦‰ì´_Tree_4.png){: .align-center}





#### Validation

ì´ì œ `search.best_params_`ë¥¼ í†µí•´ GridSearchì—ì„œ ì–»ì€ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ì–»ì„ ìˆ˜ ìˆì§€ë§Œ, ë¡œì»¬ì—ëŠ” fit modelì´ ì €ì¥ë˜ì–´ìˆì§€ ì•Šìœ¼ë¯€ë¡œ ì•ì„œ ì–»ì€ result dataframeì— ì €ì¥ë˜ì–´ìˆëŠ” rankingìœ¼ë¡œ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤. best hyperparmeterì¸ `max_depth = 4`, `n_estimators = 320`ì„ ì´ìš©í•˜ì—¬ `train_test_split` ìœ¼ë¡œ ì²˜ìŒì— êµ¬í•œ Validation dataì— ëŒ€í•´ RMSEë¥¼ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

```py
# Best_params
best_params = search_res.loc[search_res['rank_test_score']==1,["param_gbm__max_depth","param_gbm__n_estimators"]]
gbdt_best = Pipeline([
("preprocessor",preprocessor),
("gbm",GradientBoostingRegressor(
    loss='squared_error',max_features='auto',
    max_depth=best_params.iloc[0,0], n_estimators=best_params.iloc[0,1]))
])
gbdt_best.fit(X_train,y_train)
rmse(y_true=y_val, y_pred=gbdt_best.predict(X_val)) # 36.827
```

ì˜¤íˆë ¤ Cross validation errorë³´ë‹¤ ë” ë‚®ì€ RMSEê°€ ë„ì¶œë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

### Random Forest

ì´ì œ ë§ˆì§€ë§‰ìœ¼ë¡œ íŠ¸ë¦¬ í™œìš© ëª¨ë¸ ì¤‘ ê°€ì¥ ì¸ê¸°(?)ìˆë‹¤ê³  í•  ìˆ˜ ìˆëŠ” [ëœë¤í¬ë ˆìŠ¤íŠ¸](https://ddangchani.github.io/machine%20learning/RandomForest/) ëª¨ë¸ì„ êµ¬í˜„í•´ë³´ë„ë¡ í•˜ì. ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ë„ ë§ˆì°¬ê°€ì§€ë¡œ íŠœë‹í•´ì•¼í•  Hyperparmeterê°€ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì—, ì ì ˆí•œ hyperparameterì„ ì„ íƒí•˜ì—¬ Gridsearchë¥¼ ì§„í–‰í•´ë³´ë„ë¡ í•˜ê² ë‹¤(*Colab ì´ìš©*). ì´ ë„¤ ê°œì˜ hyperparmeter `n_estimators`(ëœë¤í¬ë ˆìŠ¤íŠ¸ë‹¹ ê°œë³„ íŠ¸ë¦¬ì˜ ê°œìˆ˜), `max_features`, `max_depth`, `min_samples_split`(í•œ ë…¸ë“œë¥¼ splití•˜ê¸° ìœ„í•´ í•„ìš”í•œ ìµœì†Œí•œì˜ ìƒ˜í”Œ ê°œìˆ˜) ì— ëŒ€í•´ ì ë‹¹í•œ ê°’ì„ Gridë¡œ ì·¨í•˜ì—¬ í›ˆë ¨ì„ ì§„í–‰í–ˆë‹¤.

```py
# RandomForest Regressor
from sklearn.ensemble import RandomForestRegressor
rf_reg = Pipeline(steps=[('preprocessor',preprocessor),('rf',RandomForestRegressor())])
param_grid = {
    'rf__n_estimators' : [50, 100, 200, 400], # number of trees
    'rf__max_features' : ['auto','sqrt'],
    'rf__max_depth' : [5, 10, 15, 20],
    'rf__min_samples_split' : [2, 6, 10],
    'rf__bootstrap' : [True]
}
reg = GridSearchCV(rf_reg, param_grid, verbose=10, scoring=rmse_score)
reg.fit(X_train, y_train)
```

ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì„ ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•© `best_params`ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì„ íƒí•œ í›„, ì´ë¥¼ ì´ìš©í•´ ëœë¤í¬ë ˆìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸ì„ ì¬êµ¬ì„±í–ˆë‹¤.

```py
# Return best parameter of Gridsearch at RandomForest
rf_res = pd.read_csv('logs/rf_grid_res.csv',index_col=0)
best_params = rf_res.loc[rf_res.rank_test_score==1,[i for i in rf_res.columns if 'param_' in i]]
n_params = []
for i in best_params.columns:
    n_params.append(i[10:])
best_params.columns = n_params
best_params = best_params.to_dict('records')[0]
# RF with best params
rf_best= Pipeline(steps=[('preprocessor',preprocessor),('rf',RandomForestRegressor(**best_params))]) # ** as kwargs unpacking
rf_best.fit(X_train, y_train)
```

ë˜í•œ, ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ì˜ íŠ¹ì§•ìœ¼ë¡œ íšŒê·€ë¶„ì„ì²˜ëŸ¼ ë³€ìˆ˜ì˜ ì¤‘ìš”ë„ë¥¼ íŠ¹ì • ê¸°ì¤€ì— ì˜í•´ ì¸¡ì •í•  ìˆ˜ ìˆëŠ”ë°, ë‹¤ìŒê³¼ ê°™ì´ Pipelineì˜ randomforest ëª¨í˜•ì—ì„œì˜ attribute `rfreg.feature_importances_`ë¡œ ê·¸ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆê³ , ì´ë¥¼ bar chartë¡œ ê°„ë‹¨íˆ í‘œí˜„í•´ë³´ì•˜ë‹¤.

```py
# Variable Importance
rfreg = rf_best.named_steps['rf']
idx= list(X_train.columns.drop('precip'))
idx.extend(['precip_0','precip_1'])
importance = pd.DataFrame(rfreg.feature_importances_[:-1], idx).round(3)
```

![rf_variable_importance](/assets/img/ë”°ë¦‰ì´_Tree_5.png){: .align-center}

ë§ˆì§€ë§‰ìœ¼ë¡œ, ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨í˜•ì— ëŒ€í•´ì„œë„ Validation dataì— ëŒ€í•œ RMSEë¥¼ êµ¬í•´ë³¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì´ ê½¤ ê´œì°®ì€ ê°’ì„ ì–»ì„ ìˆ˜ ìˆì—ˆë‹¤.

```py
# Validation
pred_rf = rf_best.predict(X_val)
rmse(y_val, pred_rf) # 35.882
```

- ğŸ–¥ Full code on Github : https://github.com/ddangchani/project_ddareungi

{% endraw %}