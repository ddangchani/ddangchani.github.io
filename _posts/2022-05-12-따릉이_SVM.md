---
title: "ë”°ë¦‰ì´ ë°ì´í„° ë¶„ì„í•˜ê¸° (6) Support Vector Machine"
tags:
- Project
- SVM
- Python
category: Project
use_math: true
header: 
 teaser: /assets/img/ë”°ë¦‰ì´_SVM_0.png
---
{% raw %}
## ë”°ë¦‰ì´ ë°ì´í„° ë¶„ì„í•˜ê¸° (6) Support Vector Machine

ì´ë²ˆ ê¸€ì—ì„œëŠ” ëŒ€í‘œì ì¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì¸ SVM(Support Vector Machine)ì„ ì´ìš©í•´ ë”°ë¦‰ì´ ì´ìš© ë°ì´í„°ì˜ ë¶„ì„ì„ ì§„í–‰í•´ë³´ë„ë¡ í•˜ì. ë³¸ë˜ SVMì€ classificationì˜ ëª©ì ì„ ìœ„í•´ ê³ ì•ˆëœ ê¸°ë²•ìœ¼ë¡œ, ë°ì´í„°ë“¤ì˜ ë ˆì´ë¸”ì„ ë¶„ë¥˜í•˜ëŠ” ê¸°ì¤€ì´ ë˜ëŠ” ì´ˆí‰ë©´ì„ ì°¾ì•„ë‚´ëŠ” ê³¼ì •ì´ë‹¤. ê·¸ëŸ°ë° ì´ ê³¼ì •ì˜ ì•„ì´ë””ì–´ë¥¼ ì´ìš©í•´ SVMì„ íšŒê·€ ë¬¸ì œì—ì„œë„ ì‚¬ìš©í•˜ê²Œë” í•  ìˆ˜ ìˆëŠ”ë°, ì—¬ê¸°ì„œ ë‹¤ë£° ë¬¸ì œëŠ” íšŒê·€ë¬¸ì œ ì´ë¯€ë¡œ ì´ë¥¼ ì´ìš©í•´ë³´ê³ ì í•œë‹¤.([ì°¸ê³ ](https://ddangchani.github.io/machine%20learning/Support_Vector_Machine/)) 

Scikit-learnì—ëŠ” `sklearn.svm` ëª¨ë“ˆë¡œ ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹  ëª¨ë¸ë“¤ì„ ì œê³µí•œë‹¤. íšŒê·€ ë¬¸ì œì˜ ê²½ìš° `SVR`(Support Vector Regressor)ì„ ì‚¬ìš©í•˜ë©´ ë˜ëŠ”ë° ì´ì™¸ì—ë„ `LinearSVR`,`NuSVR`ê³¼ ê°™ì€ ë‹¤ë¥¸ í˜•íƒœì˜ SVM regressorì„ ì œê³µí•˜ê³  ìˆë‹¤. ìš°ì„  ê°€ì¥ ê¸°ë³¸ì ì¸ SVR ëª¨ë“ˆì„ ì´ìš©í•´ íšŒê·€ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ë„ë¡ í•˜ì.

### SVR

ì´ì „ì— ì‚´í´ë³¸ ë‹¤ë¥¸ ëª¨ë¸ë“¤ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, `sklearn`ì˜ `Pipeline`ì„ ì´ìš©í•´ ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ êµ¬ì„±í•  ê²ƒì¸ë°, ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ì „ì²˜ë¦¬ ê³¼ì •ì€ ì´ì „ì— ì‚¬ìš©í•œ ì½”ë“œì™€ ìœ ì‚¬í•˜ë‚˜ SVMì˜ ê²½ìš° Input dataì˜ ìŠ¤ì¼€ì¼(scale)ì— ì˜í–¥ì„ ë°›ê¸° ë•Œë¬¸ì— ì—¬ê¸°ì„œëŠ” `hour` ë³€ìˆ˜ê¹Œì§€ `StandardScaler()`ë¡œ ì²˜ë¦¬í–ˆë‹¤(*í¸ì°¨ë¥¼ í‘œì¤€í¸ì°¨ë¡œ ë‚˜ëˆ ì£¼ëŠ” ê³¼ì •ì´ í¬í•¨ë˜ì—ˆë‹¤ëŠ” ì˜ë¯¸ì„*). ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìš°ì„  ë””í…Œì¼í•œ hyperparameterì˜ ì¡°ì • ì—†ì´, SVMì—ì„œ ì‚¬ìš©í•  ì»¤ë„ì„ ê²°ì •í•˜ê¸° ìœ„í•´ ì„¸ ê°€ì§€ ì»¤ë„ì„ ë°”íƒ•ìœ¼ë¡œ scoreë“¤ì„ ë¹„êµí•´ë³´ë„ë¡ í•˜ì. ì—¬ê¸°ì„œëŠ” RBF, Linear, Polynomial kernel(degree = 3)ì˜ ì„¸ ê°€ì§€ë¥¼ ì´ìš©í–ˆë‹¤. `gamma`, `epsilon`, `C` ë“±ì˜ ì—¬íƒ€ hyperparmeterëŠ” ì»¤ë„ì„ ê²°ì •í•œ ì´í›„ì— Gridsearchë¡œ ì¡°ì ˆí•´ë³´ë„ë¡ í•˜ì. ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

```python
# 3 kernel SVR models
svr_rbf = Pipeline(
    steps=[("preprocessor",preprocessor),
    ("svr",SVR(kernel="rbf", C=100, gamma=0.1, epsilon=0.1))]
)
svr_lin = Pipeline(
    steps=[("preprocessor",preprocessor),
    ("svr",SVR(kernel="linear", C=100, gamma=0.1, epsilon=0.1))]
)
svr_poly = Pipeline(
    steps=[("preprocessor",preprocessor),
    ("svr",SVR(kernel="poly",degree=3 , C=100, gamma=0.1, epsilon=0.1))]
)
models = [svr_rbf,svr_lin,svr_poly]
for svr in models:
    svr.fit(X_train, y_train)

# Metric for each model

for svr in models:
    y_pred = svr.predict(X_val)
    rmse = np.sqrt(mean_squared_error(y_val,y_pred))
    print('Kernel: %s  Score: %.3f' % (svr.named_steps['svr'].kernel, rmse))
    
# Kernel: rbf  Score: 44.902
# Kernel: linear  Score: 52.397
# Kernel: poly  Score: 48.031
```

ê° ì»¤ë„ì— ëŒ€í•´ `svr_rbf, svr_lin, svr_poly`ë¡œ ëŒ€ì‘í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì—ˆê³ , train dataë¥¼ ì´ìš©í•´ ëª¨ë¸ì„ fitting í•œ í›„, validation dataì— ëŒ€í•œ RMSE scoreë¥¼ ê°ê° ì¶œë ¥í•˜ë„ë¡ í•˜ì˜€ë‹¤. ê·¸ ê²°ê³¼(ë°‘ë¶€ë¶„ ì£¼ì„) Gaussian RBF kernelì—ì„œ ê°€ì¥ ë‚®ì€ RMSEë¥¼ ì–»ì„ ìˆ˜ ìˆì—ˆëŠ”ë°, ì–´ë–¤ í˜•íƒœë¡œ íšŒê·€ëª¨ë¸ì´ í˜•ì„±ë˜ì—ˆëŠ”ì§€ ì‹œê°ì ìœ¼ë¡œ ì‚´í´ë³´ê¸° ìœ„í•´ ì´ë²ˆì—ëŠ” ì´ì „ì— ë‹¤ë£¨ì–´ë³¸ ê²ƒ ì²˜ëŸ¼ PCAë¥¼ ì´ìš©í•´ ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ì—ì„œì˜ ì •ì‚¬ì˜ì„ í†µí•´ íšŒê·€ê°€ ì´ë£¨ì–´ì§€ëŠ” ì–‘ìƒì„ ì‚´í´ë³´ë„ë¡ í•˜ì.

#### SVR with PCA

```python
# SVR with PCA and Plot
from sklearn.decomposition import PCA
svr_rbf = Pipeline(
    steps=[("preprocessor",preprocessor),
    ('pca',PCA(n_components=3)),
    ("svr",SVR(kernel="rbf", C=100, gamma=0.1, epsilon=0.1))]
)
svr_lin = Pipeline(
    steps=[("preprocessor",preprocessor),
    ('pca',PCA(n_components=3)),
    ("svr",SVR(kernel="linear", C=100, gamma=0.1, epsilon=0.1))]
)
svr_poly = Pipeline(
    steps=[("preprocessor",preprocessor),
    ('pca',PCA(n_components=3)),
    ("svr",SVR(kernel="poly",degree=3 , C=100, gamma=0.1, epsilon=0.1))]
)
models = [svr_rbf, svr_lin, svr_poly]
kernel_labels = ['Gaussian RBF','Linear','Polynomial']

fig, axes = plt.subplots(nrows=3, ncols=1, figsize = (10,15))
cmap = ['darkgreen', 'navy', 'magenta']
for idx, svr in enumerate(models):
    svr.fit(X_train, y_train)
    pca = svr[0:2] # Pipeline until PCA
    xs, ys = zip(*sorted(zip(pca.transform(X_val)[:,0],svr.predict(X_val)))) # projected X and predicted y
    axes[idx].scatter(pca.transform(X_val)[:,0], y_val, alpha = 0.4, label='training data')
    axes[idx].plot(xs, ys, label = 'predicted data with kernel = ' + kernel_labels[idx], color=cmap[idx], linewidth = 2)
    axes[idx].legend(fontsize = 12, loc='upper right')
    axes[idx].set(xlabel = 'First Principal Component', ylabel='Y')

fig.suptitle("Support Vector Regression with Kernels", fontsize = 14)
plt.savefig("plots/SVR_with_PCA_by_kernel.png", facecolor = 'white', transparent = False)
```

![SVR_with_PCA_by_kernel](/assets/img/ë”°ë¦‰ì´_SVM_0.png){: .align-center}

ìœ„ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ë°”ë¡œ ìœ„ì˜ plotì„ ì–»ì„ ìˆ˜ ìˆëŠ”ë°, ë¶„ë¥˜ ë¬¸ì œê°€ ì•„ë‹Œ íšŒê·€ ë¬¸ì œì´ë¯€ë¡œ ê° ì»¤ë„ì´ ì–´ë–»ê²Œ ì‘ë™í–ˆëŠ”ì§€ ëª…í™•íˆ íŒŒì•…í•˜ëŠ” ê²ƒì´ ë‹¤ì†Œ ì–´ë µê³ , ê³¼ì í•© ìœ ë¬´ë„ ì‹œê°ì ìœ¼ë¡œëŠ” ëª…í™•íˆ êµ¬ë¶„ë˜ì§€ ì•ŠëŠ”ë‹¤. ë”°ë¼ì„œ ì—¬ê¸°ì„œëŠ” ê·¸ëƒ¥ Validation RMSEê°€ ê°€ì¥ ë‚®ê²Œ ë„ì¶œëœ RBF kernelì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬, ì´ì œ ì„¸ë¶€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ìœ¨í•´ë³´ë„ë¡ í•˜ê² ë‹¤.

#### Hyperparameter Tuning

ì´ë²ˆì—ëŠ” RBF kernelì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ë¥¸ hyperparmeterë“¤ì„ gridsearchë¡œ ì¡°ìœ¨í•´ë³´ë„ë¡ í•˜ì. ìš°ì„  hyperparmeter `C`ëŠ”  ë‹¤ìŒì˜ Lagrangrian formì—ì„œì˜ ë¹„ìš© ìƒìˆ˜ $C$ì— ë°˜ë¹„ë¡€í•œë‹¤(*ì´ëŠ” classification SVMì˜ formì´ë‹¤([ì°¸ê³ ](https://ddangchani.github.io/machine%20learning/Support_Vector_Machine/))*).

$$

\beta,\beta_0 = \arg\min{1\over 2}\Vert\beta\Vert^2+C\sum_{i=1}^N\xi_i\\
\text{subject to}\;\;\xi_i\geq 0,y_i(x_i^T\beta+\beta_0)\geq 1-\xi_i\tag{0}

$$

$C>0$ì˜ ê°’ì€ ê¸°ë³¸ê°’ 1ë¡œ ì„¤ì •ë˜ì–´ìˆëŠ”ë°, $C$ ê°’ì´ ì»¤ì§€ë©´($C\uparrow+\infty$) ê° slack variable $\xi_i$ ë“¤ì´ 0ì— ìˆ˜ë ´í•˜ì—¬ ê²°ê³¼ì ìœ¼ë¡œ Hard margin classifierê°€ ë˜ê³ , ë°˜ëŒ€ì˜ ê²½ìš°ëŠ” ê·œì œê°€ ì™„í™”ëœë‹¤. `SVR()`ì—ì„œì˜ Hyperparmeter `C`ëŠ” ì´ì™€ ë°˜ëŒ€ë¡œ ê°’ì´ ì‘ì•„ì§ˆìˆ˜ë¡ ê°•í•œ ê·œì œê°€ ì‘ë™í•˜ëŠ” ë°©ì‹ì´ë‹¤,

ë‹¤ë¥¸ hyperparmeterì¸ `epsilon`ì€ ì„¤ì •ëœ ê°’ ì´í•˜ì˜ ì˜¤ì°¨ë¥¼ ë¬´ì‹œí•˜ëŠ” support vector regressionì—ì„œì˜ ì•Œê³ ë¦¬ì¦˜ì„ ì˜ë¯¸í•˜ëŠ”ë°, ìì„¸í•œ ë‚´ìš©ì€ ìœ„ ë§í¬ì˜ SVR ë‚´ìš©ì„ ì°¸ê³ í•˜ë©´ ëœë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ hyperparmeter `gamma`ëŠ” ì»¤ë„í•¨ìˆ˜(rbf, sigmoid, polynomialì˜ ê²½ìš°ë§Œ ì¡´ì¬í•œë‹¤.)

$$

K(x,x') = \exp(-\gamma\Vert x-x'\Vert^2)

$$

ì— í¬í•¨ëœ $\gamma$ ê°’ì„ ì˜ë¯¸í•œë‹¤. `gamma = 'auto'` ë¡œ ì„¤ì •ëœ ê²½ìš° ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ê°’ì¸ $1/p$ ($p$ ëŠ” input dataì˜ íŠ¹ì„± ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤) ë¥¼ ì‚¬ìš©í•œë‹¤.

í•´ë‹¹ hyperparmeterë“¤ì— ëŒ€í•œ gridë¥¼ exponential í˜•íƒœë¡œ ì„¤ì •í–ˆìœ¼ë©°, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ì½”ë“œë¥¼ í†µí•´ GridSearchë¥¼ ì§„í–‰í•˜ì˜€ë‹¤.

```py
# Hyperparameter Tuning with GridSearch
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer
# RMSE def for Gridsearch Scoring
def rmse(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    return(np.sqrt(mse))
rmse_score = make_scorer(rmse, greater_is_better=False)

# Model and grid setting
svr = Pipeline(
    steps=[("preprocessor",preprocessor),("svr",SVR())]
)
param_grid = {
    'svr__C' : np.logspace(-2,2,5), # Regularization
    'svr__kernel' : ['rbf'],
    'svr__epsilon' : np.logspace(-3,0,4), # epsilon at SVM regressor
    'svr__gamma' : ['auto', 0.1, 0.01]
}
svr_grid = GridSearchCV(svr, param_grid, verbose=2, scoring=rmse_score)
svr_grid.fit(X_train, y_train)
```

ìµœì ì˜ hyperparmeter ì¡°í•©ì„ ë‹¤ìŒê³¼ ê°™ì´ ì°¾ì•„ë‚´ê³  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ Validation dataì— ëŒ€í•œ RMSE errorë¥¼ êµ¬í•œ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ë‹¤.

```py
# Gridsearch result
from copy import deepcopy
best_params = deepcopy(svr_grid.best_params_)
for key in param_grid.keys():
    best_params[key[5:]] = best_params.pop(key)
print(best_params) 
# {'C': 100.0, 'kernel': 'rbf', 'epsilon': 1.0, 'gamma': 0.1}

# Best SVR model
svr_best = Pipeline(steps=[('preprocessor',preprocessor),('svr',SVR(**best_params))])
svr_best.fit(X_train, y_train)
# Validation RMSE
y_pred = svr_best.predict(X_val)
rmse(y_val, y_pred).round(3) # 44.871
```

ì²˜ìŒì— ì„ì˜ë¡œ ì„¤ì •í–ˆë˜ hyperparmeter ì¡°í•©ì— ë¹„í•´ validation errorê°€ ê±°ì˜ ê°œì„ ë˜ì§€ ëª»í–ˆìœ¼ë©°, íŠ¹íˆ ê·œì œ hyperparmeter $C$ì˜ ê²½ìš° ê°€ì¥ í° ê°’ì„ ì·¨í•˜ë ¤í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì•„ SVM ì•Œê³ ë¦¬ì¦˜ì—ì„œì˜ ê·œì œ ìì²´ê°€ ìœ ì˜ë¯¸í•˜ê²Œ ì‘ë™í•˜ì§€ ëª»í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë˜í•œ, ì—¬ê¸°ì„œ ì‚¬ìš©ëœ SVR ë°©ë²•ì€ ê¸°ë³¸ì ì¸ $\epsilon$-SVRë¡œ, í•˜ì´í¼íŒŒë¼ë¯¸í„°ì¸ $\epsilon$ì€ ëª¨ë¸ì´ ì˜¤ì°¨ë¥¼ ì–¼ë§ˆë‚˜ ìœ ì—°í•˜ê²Œ ë°›ì•„ë“¤ì¼ ì§€(tolerate)ì— ëŒ€í•œ ê°’ì´ë‹¤([ì°¸ê³ ](https://ddangchani.github.io/machine%20learning/Support_Vector_Regression/)). ê·¸ëŸ¬ë‚˜, ìœ„ Gridsearch ê³¼ì •ì—ì„œ epsilon ê°’ì€ gridì¤‘ ê°€ì¥ í° ê°’(ì—¬ê¸°ì„œëŠ” $1.0$)ì„ íƒí•˜ë ¤ëŠ” ê²½í–¥ì„ ë³´ì˜€ë‹¤. ì¦‰, ì•ì„œ ì–»ì€ SVR ëª¨ë¸ì€ $1.0$ ì´í•˜ì˜ ì˜¤ì°¨ì— ëŒ€í•´ insensitiveí•œ ëª¨ë¸ì´ë‹¤. ê·¸ëŸ°ë°, ìš°ë¦¬ëŠ” ì•ì„œ ìœ„ ëª¨ë¸ì˜ ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ response variable $Y$(ì´ìš© íšŸìˆ˜ : count)ì— ëŒ€í•´ í‘œì¤€í™” ìŠ¤ì¼€ì¼ì„ ì ìš©í•œ ë°” ìˆë‹¤.

ë”°ë¼ì„œ, ìŠ¤ì¼€ì¼ëœ ë°˜ì‘ë³€ìˆ˜ì— ëŒ€í•´ 1.0 ì´í•˜ì˜ ì˜¤ì°¨ë¥¼ ë¬´ì‹œí•œë‹¤ëŠ” ê²ƒì€ ë°ì´í„°ì…‹ì˜ 1í‘œì¤€í¸ì°¨ë§Œí¼ì˜ ì˜¤ì°¨ë¥¼ ë¬´ì‹œí•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì •ê·œë¶„í¬ ê°€ì •í•˜ì— ê´€ì¸¡ê°’ìœ¼ë¡œë¶€í„° 1í‘œì¤€í¸ì°¨ì˜ ë²”ìœ„ëŠ” 68%ì˜ ê´€ì¸¡ê°’ ë¹„ìœ¨ì„ ì˜ë¯¸í•˜ë¯€ë¡œ, ì´ëŠ” ì‚¬ì‹¤ìƒ SVR ë©”ì»¤ë‹ˆì¦˜ì´ ë¬´ì˜ë¯¸í•  ì •ë„ë¡œ í˜•í¸ì—†ì´ ë™ì‘í•œë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.

### NuSVR

ì´ë²ˆì—ëŠ” SVRì˜ ë³€í˜• í˜•íƒœì¤‘ í•˜ë‚˜ì¸ NuSVR([ì°¸ê³ ](https://ddangchani.github.io/machine%20learning/Support_Vector_Regression/)) ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ë¶„ì„ì„ ì§„í–‰í•´ë³´ë„ë¡ í•˜ê² ë‹¤. NuSVRì—ì„œëŠ” ë‹¤ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì¸ Nu($\nu$) ê°’ì„ ì§€ì •í•´ì£¼ì–´ì•¼ í•˜ëŠ”ë°, $\nu$ ê°’ìœ¼ë¡œ epsilonê°’ì„ ì»¨íŠ¸ë¡¤í•˜ê¸° ë•Œë¬¸ì— ì—¬ê¸°ì„œëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° epsilonì„ ë³„ë„ë¡œ ì§€ì •í•´ì¤„ í•„ìš” ì—†ë‹¤. ì´ë²ˆì—ë„ ì•ì„  ê²½ìš°ì™€ ë§ˆì°¬ê°€ì§€ë¡œ Gridsearchë¥¼ ì´ìš©í•´ hyperparmeter tuningì„ ì§„í–‰í•´ë³´ë„ë¡ í•˜ê² ë‹¤.

```py
# NuSVR
from sklearn.svm import NuSVR
nusvr = Pipeline(
    steps=[("preprocessor",preprocessor),("svr",NuSVR())]
)
param_grid = {
    'svr__nu' : [0.01, 0.1, 0.5, 0.75],
    'svr__C' : np.logspace(-2,2,5), # Regularization
    'svr__kernel' : ['rbf','linear'],
    'svr__gamma' : ['auto', 0.1, 0.01]
}
nusvr_grid = GridSearchCV(nusvr, param_grid, verbose=2, scoring=rmse_score)
# Fit
nusvr_grid.fit(X_train, y_train)
```

ì•ì„œ SVRì˜ `best_params`ë¥¼ êµ¬í•˜ëŠ” ì½”ë“œë¥¼ ì´ìš©í•´ ë‹¤ìŒê³¼ ê°™ì´ best parameterë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

```py
{'nu': 0.5, 'C': 100.0, 'kernel': 'rbf', 'gamma': 0.1}
```

nuëŠ” default ê°’ì¸ 0.5ë¥¼ ì±„íƒí–ˆê³ , kernel ì¢…ë¥˜ì™€ gamma ê°’ë„ SVR ê²°ê³¼ì™€ ë™ì¼í–ˆë‹¤. ë‹¤ë§Œ, ì—¬ê¸°ì„œë„ C ê°’ì´ ì œì¼ í° ê°’ì„ íƒí•˜ì˜€ê¸°ì— ëª¨ë¸ì˜ ê·œì œí•­ ìì²´ê°€ ìœ ì˜ë¯¸í•˜ê²Œ ì‘ë™í•˜ì§€ ëª»í•œë‹¤ëŠ” ì‚¬ì‹¤ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ, ì´ë ‡ê²Œ ì–»ì€ best parameterë“¤ì„ ì´ìš©í•´ ë‹¤ìŒê³¼ ê°™ì´ validation scoreì„ ê³„ì‚°í–ˆë‹¤. ì‘ì€ ê°ì†ŒëŠ” ìˆì—ˆì§€ë§Œ, í¬ê²Œ ìœ ì˜ë¯¸í•˜ë‹¤ê³  ë³´ì—¬ì§€ì§€ ì•ŠëŠ”ë‹¤. ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•´ë³¼ ë•Œ Support Vector Regressionì´ ë³¸ Problemì— ì í•©í•˜ì§€ ì•ŠëŠ”ë‹¤ê³  íŒë‹¨ë˜ëŠ”ë°, ì•„ë§ˆ categorical variableì´ë‚˜, íŠ¹íˆ ì‹œê°„ëŒ€ì™€ ê°™ì€ ë³€ìˆ˜ë¡œ ì¸í•´ ê·¸ëŸ¬í•œ í˜„ìƒì´ ë°œìƒí•˜ëŠ” ê²ƒìœ¼ë¡œ ì¶”ì¸¡ëœë‹¤.

```py
# Best NuSVR model
nusvr_best = Pipeline(steps=[('preprocessor',preprocessor),('svr',NuSVR(**best_params))])
nusvr_best.fit(X_train, y_train)
# Validation RMSE
y_pred = nusvr_best.predict(X_val)
rmse(y_val, y_pred).round(3) # 43.844
```



##  References

- Scikit-Learn ê³µì‹ ë¬¸ì„œ : https://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html#sphx-glr-auto-examples-svm-plot-svm-regression-py

- ğŸ–¥ Full code on Github : https://github.com/ddangchani/project_ddareungi
{% endraw %}