---
title: "Logistic Regression"
tags:
- Linear Model
- Machine Learning
- Linear Classification
- Logistic Regression
category: Linear Model
use_math: true
---
{% raw %}
## Logistic Regression
로지스틱 회귀분석은 회귀분석의 일종으로 보기 쉽지만, 사실 분류문제의 사후확률을 선형함수로 표기한것에서 비롯되기 때문에 분류기법으로 보는 것이 더 정확하다. 즉, 각 데이터 $X=x$ 가 클래스 $G=k\;(k=1\cdots K)$ 를 가질 확률을 비율로 치환(한개 클래스를 기준으로, 여기서는 $G=K$)한 뒤   

$$

\log\frac{P(G=k\vert X=x)}{P(G=K\vert X=x)} = \beta_{k0}+\beta_k^\top x

$$

의 형태로 $k=1$ 부터 $k=K-1$까지의 로그비를 선형관계식으로 둔다 ($K-1$ 개만 두는 이유는 확률들의 합이 1이 되는 제약조건으로부터 자유롭게 하기 위함이다). 그러면, 특정 클래스에 대해   

$$

P(G=k\vert X=x) = \frac{exp(\beta_{k0}+\beta_k^\top x)}{1+\sum_{l=1}^{K-1} exp(\beta_{l0}+\beta_l^\top x)}

$$   

임을 계산으로부터 알 수 있다. 편의를 위해 모수집합을   

$$

\theta = \{\beta_{10},\beta_1,\ldots,\beta_{(K-1)0},\beta_{K-1}\}

$$

으로 두고, 각 확률을   

$$

P(G=k\vert X=x) = p_k(x;\theta)

$$   

로 표기하자.   

### 모수추정과 적합
로지스틱 회귀모형을 추정하는 것은 대개 최대가능도추정량<sup>maximum likelihood estimator</sup>을 이용한다. 우선 N개의 관측값에 대한 로그가능도함수는    

$$

l(\theta) = \sum_{i=1}^N\log p_{g_i}(x_i;\theta)

$$   

로 주어진다. 계산과 이해의 편의를 위해 이변수 경우를 다루어보자. $g_i=1$ 일 때 $y_i=1$이고 $g_i=2$ 일 때 $y_i=0$으로 주어진다고 하자. 또한, $p_1(x)=p(x)$ 로 두고, $p_2(x) = 1-p(x)$ 로 두자. 그러면 모수는 $\beta^\top=(\beta_{10},\beta_1)$ 이므로 로그가능도함수는    

$$\begin{aligned}

l(\beta)&=\sum_{i=1}^N\{y_i\log p(x_i;\beta)+(1-y_i)\log(1-p(x_i;\beta))\}    \\
&= \sum_i^N\{y_i\beta^\top x_i-\log(1+e^{\beta^\top x_i})\}
\end{aligned}

$$   

로 쓸 수 있다.    
가능도함수를 최대화하기 위해 양변을 $\beta$ 로 미분하면   

$$

\frac{\partial l(\beta)}{\partial\beta} = \sum_{i=1}^N x_i(y_i-p(x_i;\beta)) = 0

$$    

이 된다. 즉, 주어진 데이터셋이 $p$차원이면 위 식은 $p+1$ 개의 비선형 방정식이 된다. 따라서 closed form의 해를 구할 수 없으므로, 주로 Newton Method(또는 Newton-Raphson Method)를 이용해 해를 구한다. 이때 Newton Method는 초기값 $\beta^{old}$ 으로부터   

$$

\beta^{new} = \beta^{old} - \biggl(\frac{\partial^2 l(\beta)}{\partial\beta\partial\beta^\top}\biggr)^{-1}\frac{\partial l(\beta)}{\partial\beta}

$$

로 개선되어지는 방식을 통해 구해진다. 이때 초기값은 임의로(ex. $\beta=0$) 설정되지만 global maximum이 아닌 local maximum에 빠질 우려가 있으므로 여러개의 초기값을 설정하여 검토해야 한다.

### 규제항이 추가된 로지스틱 회귀
회귀분석에서의 [Lasso](https://ddangchani.github.io/linear%20model/linearreg1/)와 유사하게, 로지스틱 회귀모형에도 L1 규제항을 추가할 수 있다. 즉, 로그가능도함수에 다음과 같이 규제항을 추가한다.   

$$

l^{LASSO}(\beta) = \sum_{i=1}^N [y_i(\beta_0+\beta^\top x_i)-\log(1+e^{\beta_0+\beta^\top x_i})] - \lambda\sum_{j=1}^p\vert\beta_j\vert

$$

위 식을 최대화하는 모수 $\beta$를 찾으면 되는데, 비선형적인 프로그래밍으로 해결하거나 혹은 앞서 살펴본 Newton Method 등과 같은 근사법을 이용해 해결할 수 있다.

## 로지스틱 회귀와 LDA의 비교
LDA를 살펴보는 과정에서 우리는 사후확률의 로그비가 다음과 같이 나타남을 확인했었다(K개의 클래스 분류문제).   

$$\begin{aligned}

\log\frac{P(G=k\vert X=x)}{P(G=K\vert X=x)}
&= \log\frac{\pi_k}{\pi_K}-\frac{1}{2}(\mu_k+\mu_K)^\top\Sigma^{-1}(\mu_k-\mu_K)+x^\top\Sigma^{-1}(\mu_k-\mu_K)\\
&= \alpha_{k0}+\alpha_k^\top x    
\end{aligned}

$$   

이때, 위 로그비가 선형형태로 나타나는 이유는 정규성 가정에서 각 클래스들의 공분산행렬이 모두 동일하다고 가정했기 때문이다. 반면, 위에서 살펴본 것과 같이 로지스틱회귀의 사후확률 로그비 역시 선형으로 나타난다.   

$$

\log\frac{P(G=k\vert X=x)}{P(G=K\vert X=x)} = \beta_{k0}+\beta_k^\top x

$$

그렇다면, 우리는 LDA와 로지스틱 회귀가 동일한 방식이라고 생각할 수 있다. 두 방법 모두 사후확률의 로그비를 이용하고, 그것들이 각각 모두 linear하게 나타나기 때문이다. 하지만, 두 방법에는 근본적인 차이가 있는데, 바로 모수들을 추정하는 방식이다.   
LDA와 로지스틱 회귀 모두, 위의 식을 변형하여 다음과 같은 형태를 얻을 수 있다(단, 마지막 클래스 K는 임의로 설정한다).   

$$

P(G=k\vert X=x) = \frac{e^{\beta_{k0}+\beta_k^\top x}}{1+\sum_{l=1}^{K-1}e^{\beta_{l0}+\beta_l^\top x}}

$$

이때, 다음과 같은 결합확률밀도함수의 분리를 생각하면   

$$

P(X,G=k) = P(X)P(G=k\vert X)

$$

LDA에서는 P(X), 즉 데이터들의 사전확률분포를 상대도수인 추정값으로 대신해서 계산했었다. 하지만 로지스틱회귀에서는 $K$개의 사후확률비의 합이 1이 된다는 사실로부터 K-1개의 모수쌍을 만들었고, 이를 통해 사전확률분포 없이도 추정량을 구할 수 있게 되었다.   
즉, LDA는 로지스틱회귀보다 추가적인 가정(사전확률분포)을 가지며, 이는 모수들에 대한 더 많은 정보를 의미한다. 결과적으로 추정 과정에서 LDA는 로지스틱회귀보다 낮은 분산이 발생한다.


# References
 - Elements of Statistical Learning
{% endraw %}