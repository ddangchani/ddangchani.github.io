---
title: "Entropy"
tags:
- Machine Learning
- Information Theory
- Loss function
category: Machine Learning
use_math: true
---
{% raw %}
## Entropy and its related loss

### Entropy

엔트로피는 확률론에 기반한 정보이론에서 매우 중요하게 쓰이는 개념이다. 확률변수<sup>random variable</sup> $\xi$ 가([random element](https://ddangchani.github.io/Random-elements) 참고) density $f\geq 0$ 을 가진다고 하자 (✅ density function은 [Radon-Nikodym](https://ddangchani.github.io/Absolute-Continuity) 참조). 이때 $\xi$의 엔트로피는 다음과 같이 정의된다.

$$

H(\xi)=\mathbb E(-\log f(\xi)) =\int_\mathbb R \log f(x)\cdot f(x)dx

$$

기댓값<sup>expectation</sup> $\mathbb E$ 는 확률변수 $\xi\in L^1(P)$ 에 대해

$$

\mathbb E[\xi] = \int_\Omega\xi dP=\int_\mathbb R xd(P\circ\xi^{-1})(x)

$$

와 같이 정의된다. (추후에 자세히 다룰 예정)

또한, 수리통계에서 어떤 확률밀도함수의 모수 $\theta$ 에 대해 Kullback-Leibler divergence를 정의한 적이 있다. 이를 확률론에서는 더 확장하여 다음과 같이 정의한다.

#### KL divergence

Measurable space $(X,\mathcal X)$ 에서의 두 $\sigma$-finite measure $\mu,\nu$ 가 있으며 $\nu\ll\mu$ 를 만족한다고 하자. 이때 $\mu$에 대한 $\nu$의 Kullback-Leibler divergence는

$$

D_{KL}(\nu\Vert\mu) = \int_X\log(\frac{d\nu}{d\mu})d\nu

$$

로 정의된다. 특별히 Probability Space $(X,\mathcal X , P)$ 에서 정의된 **discrete** probability distribution $P,Q$ 에 대해서

$$

D_{KL}(P\Vert Q)=\sum_{x\in\mathcal X}P(x)\log\biggl(\frac{P(x)}{Q(x)}\biggr)\tag{1}

$$

으로 정의된다.

### Cross Entropy

Cross entropy는 *cross*의 의미 그대로 두 random variable가 교차할 때의 엔트로피를 의미하며, random variable $p,q$ 에 대해 다음과 같이 정의된다.

$$

H(p,q) = -\mathbb E_p[\log q]

$$

여기서 기댓값에 붙어있는 subscript $p$는 기댓값을 계산하는 과정에서 $q$의 density function이 아닌, $p$의 density function을 이용한다는 것을 의미한다. 이때 식 (1)로부터

$$

\begin{aligned}
D_{KL}(P\Vert Q)&=\sum_{x\in\mathcal X}P(x)\log\biggl(\frac{P(x)}{Q(x)}\biggr)\\
&= -\sum P(x)\log Q(x) + \sum P(x)\log P(x)
\end{aligned}

$$

이 성립하므로, cross entropy(첫째 항), entropy(둘째 항), KL divergence 사이에는 다음과 같은 관계식이 성립한다.

$$

H(p,q) = H(p) + D_{KL}(p\Vert q)

$$

만일 Probability space $(X,\mathcal X)$ 에서 정의되는 확률분포 $p,q$ 가 discrete한 경우, cross entropy를

$$

H(p,q) = -\sum_{x\in\mathcal X}p(x)\log q(x)

$$

로 쓸 수 있다.

### Cross-entropy loss function

Cross entropy는 머신러닝에서 주로 이진(혹은 다중) 분류의 손실함수로 사용된다. 어떤 분류 문제에서 레이블을 예측하는 경우, 실제 레이블($p_i$, true probability)와 에측 레이블($q_i$, predicted proability) 가 주어지므로 이 둘 간의 격차를 측정하는 도구로 앞서 설명한 cross entropy를 이용한다.

 $N$개 데이터를 갖는 데이터셋으로 $K$개의 클래스를 분류하는 다항 분류문제가 주어진다고 하자. 이때 분류기는 주어진 input data($x_i$)를 입력하여 이 데이터가 각 **클래스에 해당할 확률** $P(Y=k\vert X=x_i)$ 을 구하는 것이다($k=1,\ldots,K$). 이때 임의의 모델 $f(x_i;\theta)$를 이용해 계산한 $x_i$ 각 클래스별 출력값($\hat y_k$)은 확률이 아니므로(ex. 세 클래스에 대해 5.1, 2.7, -12의 값이 나왔다고 하자.), 우리는 이를 확률처럼 사용하기 위해 처리해주어야 한다. 이를 위해 softmax function을 사용하고, 이는 다음과 같다.

$$

P(Y=k\vert X=x_i)=\frac{e^{\hat y_k}}{\sum_je^{\hat y_j}}

$$

(*만일 이진 분류(0 또는 1)의 경우 softmax의 특수한 케이스인 sigmoid function을 사용한다*) 이렇게 처리한 값들은 이제 확률로 사용할 수 있으므로 이를 바탕으로 다음의 cross-entropy 손실함수를 계산한다($y_k^{(i)}$ 는 $i$번째 데이터가 $k$번째 클래스를 가지면 1, 아니면 0으로 계산한다).

$$

J(\theta) = -{1\over N}\sum_{i=1}^N\sum_{k=1}^Ky_k^{(i)}\log\hat y_k^{(i)}

$$

✅ *손실함수가 Sample Mean의 형태인 것은 데이터셋의 원래 확률분포를 알 수 없기 때문임을 잊지 말자!*





# References

- Foundations of Modern Probabiltiy, O.Kallenberg
- https://en.wikipedia.org/wiki/Cross_entropy
- Lecture note of "Deep Learning for Compuer Vision", UMich EECS
- Hands on Machine Learning, 2e.
{% endraw %}