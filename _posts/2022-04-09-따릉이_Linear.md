---
title: "ë”°ë¦‰ì´ ë°ì´í„° ë¶„ì„í•˜ê¸° (2) Linear Regression"
tags:
- Project
- Linear Regression
- Python
category: Project
use_math: true
header: 
 teaser: /assets/img/ë”°ë¦‰ì´_Linear_3.png
---
{% raw %}
## ë”°ë¦‰ì´ ë°ì´í„° ë¶„ì„í•˜ê¸° (2) Linear Regression

ë¨¼ì €, ì•ì„œ ì‚´í´ë³¸ ë”°ë¦‰ì´ ë°ì´í„°ì…‹ì„ ì´ìš©í•´ ê°€ì¥ ê°„ë‹¨í•œ Linear Regression Modelì„ êµ¬í˜„í•´ë³´ë„ë¡ í•˜ì. Pythonì—ëŠ” `statsmodels`ë¼ëŠ” íŒ¨í‚¤ì§€ê°€ ìˆëŠ”ë°, ì´ëŠ” Rì—ì„œ ì‚¬ìš©í•˜ëŠ” í˜•íƒœë¡œ í†µê³„ë¶„ì„ì„ ê°€ëŠ¥í•˜ê²Œ í•´ì£¼ëŠ” íŒ¨í‚¤ì§€ì´ë‹¤([ê³µì‹ ë¬¸ì„œ](https://www.statsmodels.org/stable/index.html) ì°¸ê³ ). ì´ë¥¼ ì´ìš©í•´ ì„ í˜•ëª¨í˜•ì„ ë§Œë“¤ê³ , ì´ë¥¼ ê°œì„ ì‹œì¼œë‚˜ê°€ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ë„ë¡ í•˜ì. ë¨¼ì € í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

```py
import numpy as np
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
import matplotlib.pyplot as plt
```

### OLS

Ordinary Least Square methodë¥¼ ì´ìš©í•œ ê°€ì¥ ê°„ë‹¨í•œ ì„ í˜•ëª¨í˜•ì„ ë§Œë“¤ì–´ë³´ë„ë¡ í•˜ì. ìš°ì„ , formula ì‘ì„±ì˜ í¸ì˜ë¥¼ ìœ„í•´ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì¼ë¶€ ê°’ì´ NaNìœ¼ë¡œ ì£¼ì–´ì§€ë¯€ë¡œ, ì´ë“¤ì„ ì œê±°í•˜ê³  ë‹¤ìŒê³¼ ê°™ì´ column indexë“¤ì„ ë³€ê²½í–ˆë‹¤.

```python
# Data Load
df_train = pd.read_csv('train.csv').dropna()
df_train.head(1)
df_train.columns = ['id', 'hour', 'temp', 'precip',
       'windspeed', 'humidity', 'visibility',
       'ozone', 'pm10', 'pm2_5', 'count']
print(df_train.head(1))
print(len(df_train))
```

|      |   id | hour | temp | precip | windspeed | humidity | visibility | ozone | pm10 | pm2_5 | count |
| ---: | ---: | ---: | ---: | -----: | --------: | -------: | ---------: | ----: | ---: | ----: | ----- |
|    0 |    3 |   20 | 16.3 |    1.0 |       1.5 |     89.0 |      576.0 | 0.027 | 76.0 |  33.0 | 49.0  |

ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ìŒê³¼ ê°™ì´ OLS modelì„ ë§Œë“¤ê³  fittingì„ ì‹¤í–‰í•  ìˆ˜ ìˆë‹¤. ì´ë•Œ formulaëŠ” Rê³¼ ë¹„ìŠ·í•œ í˜•íƒœë¡œ 'ë°˜ì‘ë³€ìˆ˜ ~ ì˜ˆì¸¡ë³€ìˆ˜ 1 + ì˜ˆì¸¡ë³€ìˆ˜ 2 ...' ì˜ í˜•íƒœë¥¼ ì·¨í•˜ëŠ”ë°, ì—¬ê¸°ì„œ ì˜ˆì¸¡ë³€ìˆ˜ê°€ categorical variable ì¸ ê²½ìš°ì—ëŠ” ë³€ìˆ˜ë¥¼ C(ë³€ìˆ˜) ì˜ í˜•íƒœë¡œ ë¬¶ëŠ”ë‹¤. ì´ ëª¨ë¸ì—ì„œëŠ” precipitationì´ 0 ë˜ëŠ” 1ì˜ ê°’ì„ ê°–ëŠ” ë²”ì£¼í˜• ë³€ìˆ˜ì´ë¯€ë¡œ, dummy variableì„ ìƒˆë¡œ ë§Œë“¤ì§€ ì•Šê³  `C(precip)` í˜•íƒœë¡œ ì½”ë”©í•˜ë©´ ëœë‹¤.

```py
# OLS
model = smf.ols(
    "count ~ hour + temp + C(precip) + windspeed + humidity + visibility + ozone + pm10 + pm2_5",
    data = df_train
)
results = model.fit()
results.summary()
```

ì´ë¥¼ ì‹¤í–‰í•˜ë©´ íšŒê·€ë¶„ì„ì˜ ìš”ì•½ê²°ê³¼(summary)ë¥¼ ì•„ë˜ì™€ ê°™ì´ ì–»ì„ ìˆ˜ ìˆë‹¤.

![](/assets/img/ë”°ë¦‰ì´_Linear_0.png){: .align-center}


ê° ë³€ìˆ˜ì— ëŒ€í•œ coefficient $\beta$ ì™€ ìœ ì˜ì„±(p-value, ì—¬ê¸°ì„œëŠ” $P>\vert t\vert$) ë¿ë§Œ ì•„ë‹ˆë¼ autocorrelationì„ í™•ì¸í•˜ëŠ” Durbin-Watson statistic ë“±ì„ í™•ì¸í•  ìˆ˜ìˆë‹¤. Notes [2]ì—ì„œ ê°•í•œ multicollinearityê°€ ì¡´ì¬í•  ìˆ˜ ìˆì„ ê°€ëŠ¥ì„±ì„ ë§í•´ì£¼ê³  ìˆëŠ”ë°, ì´ëŠ” ì˜ˆì¸¡ë³€ìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„ê°€ ì¡´ì¬í•  ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•œë‹¤. 

#### VIF

ì•ì„œ ì„¤ëª…í•œ ê²ƒ ì²˜ëŸ¼ ì˜ˆì¸¡ë³€ìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„ê°€ ë°œìƒí•˜ëŠ” ê²ƒì„ **Multicolinearity**<sup>ë‹¤ì¤‘ê³µì„ ì„±</sup>ì´ë¼ê³  í•˜ëŠ”ë°, multicolinearityê°€ í•´ë‹¹ ë³€ìˆ˜ë¥¼ ì œê±°í•´ì•¼ í•  ë§Œí¼ ìœ ì˜ë¯¸í•œì§€ ê²€ì¦í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ê° ë³€ìˆ˜ì˜ **VIF**<sup>Variance Inflation Factor</sup>ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤. VIFë€ Linear Regressionì„ ì´ìš©í•œ ë°©ë²•ìœ¼ë¡œ, ìì„¸í•œ ë‚´ìš©ì„ ì„¤ëª…í•˜ì§€ëŠ” ì•Šê² ìœ¼ë‚˜ ì•„ë˜ ì‹

$$

VIF_j = \frac{1}{1-R_j^2}

$$

ìœ¼ë¡œ ì •ì˜ëœë‹¤. ì¦‰, $j$ë²ˆì§¸ ë³€ìˆ˜ì— ëŒ€í•œ VIFëŠ” $R_j^2$ì„ ë°”íƒ•ìœ¼ë¡œ ê³„ì‚°ë˜ëŠ”ë° ì´ëŠ” $j$ë²ˆì§¸ ì˜ˆì¸¡ë³€ìˆ˜ë¥¼ ë°˜ì‘ë³€ìˆ˜(Y)ë¡œ, ë‚˜ë¨¸ì§€ $j-1$ê°œì˜ ì˜ˆì¸¡ë³€ìˆ˜ë“¤ì„ Xë¡œ í•˜ëŠ” ìƒˆë¡œìš´ ì„ í˜•íšŒê·€ëª¨í˜•ì„ êµ¬ì„±í•  ë•Œ ì´ì— ëŒ€í•œ R-squared valueë¥¼ ì˜ë¯¸í•œë‹¤. ë³´í†µ 10 ì´í•˜ì˜ VIFì— ëŒ€í•´ì„œëŠ” acceptableí•˜ë‹¤ê³  íŒë‹¨í•œë‹¤.

pythonì—ì„œëŠ” `statsmodels.stats`ì— `variance_inflation_factor`ë¡œ VIFë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ë¥¼ ê°€ì§€ê³  ìˆë‹¤. ì•„ë˜ì™€ ê°™ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ Intercept(ìƒìˆ˜í•­)ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë³€ìˆ˜ë“¤ì— ëŒ€í•´ ê°ê° ë³€ìˆ˜ëª…ê³¼ VIFë¥¼ ì¶œë ¥í•  ìˆ˜ ìˆë‹¤. í…ŒìŠ¤íŠ¸ ê²°ê³¼, visibilityì˜ VIFê°€ 3.73ìœ¼ë¡œ ê°€ì¥ ë†’ê¸´ í•˜ì§€ë§Œ, ë§¤ìš° í° ê°’ì„ ê°€ì§€ì§„ ì•Šìœ¼ë¯€ë¡œ, í¬ê²Œ ë¬¸ì œë˜ì§€ëŠ” ì•Šì„ ìˆ˜ì¤€ì˜ multicolinearityë¼ê³  íŒë‹¨ë˜ì—ˆë‹¤.

```py
# VIF 
from statsmodels.stats.outliers_influence import variance_inflation_factor
for i in range(1, len(model.exog_names)):
    vif = variance_inflation_factor(model.exog, i)
    print(f'Variable : {model.exog_names[i]},   VIF : {vif.round(2)}')
```

<img src="/assets/img/ë”°ë¦‰ì´_Linear_1.png" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-04-09 á„‹á…©á„Œá…¥á†« 10.43.56" />

#### Dummy variable

ì•ì„  ëª¨ë¸ì—ì„œëŠ” `precip`, ì¦‰ ê°•ìš°ìƒíƒœë§Œ dummy variableë¡œ ì„¤ì •í•˜ì˜€ë‹¤. ì‚¬ì‹¤ ì´ì „ ê¸€ì—ì„œ ì‚´í´ë³´ì•˜ë˜ ê²ƒ ì²˜ëŸ¼ hourì— ë”°ë¥¸ ì´ìš©ëŸ‰(Y)ë„ ì¼ì •í•˜ì§€ ì•Šê³ , ì‹œê°„ëŒ€ë¼ëŠ” ê²ƒì€ ìœ ì˜ë¯¸í•˜ê²Œ continuousí•œ ë³€ìˆ˜ê°€ ì•„ë‹ˆë¯€ë¡œ ì´ë¥¼ dummy variableë¡œ ì²˜ë¦¬í•˜ëŠ” ë°©ë²• ì—­ì‹œ ê³ ë ¤í•´ë³¼ ìˆ˜ ìˆë‹¤. ì•„ë˜ ì½”ë“œëŠ” ì•ì„  OLS modelì—ì„œ hourì„ categorical variableë¡œ ì²˜ë¦¬í•œ í›„, modelì˜ `summary` ì¶œë ¥ ë‚´ìš©ì„ ë³„ë„ì˜ ì´ë¯¸ì§€(ì•„ë˜ì™€ ê°™ë‹¤)ë¡œ ì €ì¥í•˜ëŠ” ì½”ë“œì´ë‹¤.

```python
# hour as dummy_variable
model = smf.ols(
    "count ~ C(hour) + temp + C(precip) + windspeed + humidity + visibility + ozone + pm10 + pm2_5",
    data = df_train
)
results = model.fit()
results.summary()

fig, ax = plt.subplots(1,1)
plt.rc('figure', figsize=(10, 15))
plt.text(0.01, 0.05, str(results.summary()), {'fontsize': 10}, fontproperties = 'monospace') # approach improved by OP -> monospace!
plt.axis('off')
plt.savefig('model_new.png', transparent = False, facecolor = 'white')
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-04-09 á„‹á…©á„’á…® 2.18.44](/assets/img/ë”°ë¦‰ì´_Linear_2.png){: .align-center}

### Model Assessment

ì•ì„  OLS ëª¨ë¸ì€ ì „ì²´ train datasetì„ ê¸°ë°˜ìœ¼ë¡œ íšŒê·€ëª¨í˜•ì„ ë§Œë“  ê²ƒì´ë¯€ë¡œ, ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì§ì ‘ì ìœ¼ë¡œ í‰ê°€í•˜ê±°ë‚˜ ê³¼ì í•© ì •ë„ë¥¼ íŒŒì•…í•˜ê¸° ì–´ë µë‹¤. ë”°ë¼ì„œ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì¸ train data, validation dataë¥¼ ì„ì˜ë¡œ êµ¬ë¶„í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•´ validation ê³¼ì •ì„ ì¶”ê°€í•˜ë„ë¡ í•˜ê² ë‹¤. ë˜í•œ, validation assessment ê¸°ì¤€ì€ RMSEë¥¼ ì‚¬ìš©í•˜ë„ë¡ í•˜ê² ë‹¤.

ìš°ì„ , ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸ `sklearn`(scikit-learn)ì— ìˆëŠ” `train_test_split`ì„ ì‚¬ìš©í•´ train dataë¥¼ 7:3ì˜ ë¹„ìœ¨ë¡œ ë‚˜ëˆ„ì—ˆê³ , 929ê°œì˜ train data, ë‚˜ë¨¸ì§€ëŠ” validation dataë¡œ ì‚¬ìš©í–ˆë‹¤.

```py
# Model split
from sklearn.model_selection import train_test_split

X, y = df_train.iloc[:,1:-1], df_train.iloc[:,-1]
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=37)
print(len(X_train))
```

ì´í›„ ëª¨ë¸ë§ì„ ìœ„í•´ dummy variable, constant variableì„ ì²˜ë¦¬í•˜ê³  Modelingì„ í–ˆê³  ë§ˆì°¬ê°€ì§€ë¡œ ìš°ì„  `summary()`ë¥¼ í†µí•´ ì„ í˜•ëª¨í˜•ì˜ fitting ê²°ê³¼ë¥¼ í™•ì¸í•˜ë„ë¡ í•˜ì. ì—¬ê¸°ì„œ `drop_first=True`ë¡œ ì„¤ì •í•œ ì´ìœ ëŠ” dummy variableì„ precip=0ì¸ ê²½ìš°ì™€ precip=1ì¸ ê²½ìš°ë¥¼ ëª¨ë‘ ìƒì„±í•´ ë¶ˆí•„ìš”í•œ correlationì˜ í˜•ì„±ì„ ë§‰ê¸° ìœ„í•¨ì´ë‹¤.

```python
# Dummy, const treatment
X_train = pd.get_dummies(X_train, columns=['precip'], drop_first=True)
X_train = sm.add_constant(X_train)
X_val = pd.get_dummies(X_val, columns=['precip'], drop_first=True)
X_val = sm.add_constant(X_val)
# Modeling
ols = sm.OLS(y_train, X_train, missing='drop')
ols_res = ols.fit()
print(ols_res.summary())
```

ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, `sklearn.metrics`ëª¨ë“ˆì— ìˆëŠ” MSE ê³„ì‚°í•¨ìˆ˜ë¥¼ ì´ìš©í•´ RMSEë¥¼ ì•„ë˜ ì½”ë“œì™€ ê°™ì´ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

```py
# Validation
from sklearn.metrics import mean_squared_error
pred_val = ols_res.predict(X_val)
rmse = mean_squared_error(y_val, pred_val)
print(f'RMSE : {np.sqrt(rmse).round(3)}') # Result : 53.242
```

## Model Selection

ì´ì œë¶€í„°ëŠ” OLSë¥¼ ì´ìš©í•œ ê¸°ë³¸ì ì¸ ì„ í˜•ëª¨í˜•ì„ ë³€ìˆ˜ ì„ íƒì„ í†µí•´ ê°œì„ ì‹œí‚¤ëŠ” ë°©ë²•ì„ ì—°êµ¬í•´ë³´ë„ë¡ í•˜ì. ì‚¬ì‹¤ ì„ í˜•ëª¨í˜•ì´ë¼ëŠ” í•œê³„ ìƒ, ëª¨ë¸ ì„±ëŠ¥ì˜ ë¹„ì•½ì ì¸ ê°œì„ ì„ ê¸°ëŒ€í•˜ê¸°ëŠ” ì–´ë µì§€ë§Œ ë°ì´í„°ì˜ êµ¬ì¡°ë‚˜ ë³€ìˆ˜ê°„ì˜ ê´€ê³„ë¥¼ ë©´ë°€íˆ íŒŒì•…í•œë‹¤ë©´ ì–´ëŠ ì •ë„ì˜ ê°œì„ ì„ ì´ë£° ìˆ˜ ìˆë‹¤.

íšŒê·€ì— ì‚¬ìš©ë  ë³€ìˆ˜ë¥¼ ì„ íƒí•˜ëŠ” ë°©ë²•ì€ ë§¤ìš° ë‹¤ì–‘í•˜ë‹¤. ì „ì§„ì„ íƒë²•, í›„ì§„ì„ íƒë²• ë“±ì„ AIC/BIC ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” ë°©ë²•ë„ ìˆê³ , Partial regression, CCPR plot ë“±ì„ ì´ìš©í•´ íŠ¹ì • ë³€ìˆ˜ì˜ ì˜í–¥ë ¥ì„ íŒŒì•…í•˜ëŠ” ë°©ë²•ë„ ìˆë‹¤. ìš°ì„  Partial regressionë¶€í„° ì‚´í´ë³´ë„ë¡ í•˜ì(Partial Regressionì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ [ì´ ê¸€](https://ddangchani.github.io/linear%20model/Partial_Regression/)ì„ ì°¸ê³ í•˜ë©´ ë  ê²ƒì´ë‹¤).

### Partial Regression Plot

```python
fig = sm.graphics.plot_partregress_grid(ols_res)
plt.show()
```

ìœ„ ì½”ë“œë¥¼ í†µí•´ ê²©ì í˜•íƒœë¡œ ê°ê°ì˜ ë³€ìˆ˜ì™€ ë°˜ì‘ë³€ìˆ˜ ê°„ì˜ Partial Regression Plotì„ ì–»ì„ ìˆ˜ ìˆëŠ”ë°, ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ë‹¤.

![PRP](/assets/img/ë”°ë¦‰ì´_Linear_3.png){: .align-center}

ìœ„ Partial Regression Plotì„ ì‚´í´ë³´ë©´, humidity, visibility, pm2_5ì˜ ì„¸ ë³€ìˆ˜ëŠ” ë¹„êµì  ë¬´ì˜ë¯¸í•œ ì˜í–¥ì„ ê°–ëŠ” ì˜ˆì¸¡ë³€ìˆ˜ë¡œ íŒŒì•…í•  ìˆ˜ ìˆë‹¤.

### CCPR

CCPR(Component-Component plus Residual) plotë„ partial regression plotê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ê° ê°œë³„ ë³€ìˆ˜ì˜ ì˜í–¥ë ¥ì„ í™•ì¸í•˜ê¸° ìœ„í•œ plotì´ë‹¤. íšŒê·€ëª¨í˜•

$$

Y=X_1\hat\beta_1 +\cdots+X_p\hat\beta_p+\epsilon

$$

ì´ ì£¼ì–´ì§ˆ ë•Œ ê°œë³„ ë³€ìˆ˜ $X_i$ë¥¼ ê°€ë¡œì¶•ìœ¼ë¡œ, $X_i\hat\beta_i+\epsilon$ ì„ ì„¸ë¡œì¶•ìœ¼ë¡œ ê·¸ë¦° scatter plotì´ ë°”ë¡œ CCPR plotì´ë‹¤. ì¦‰, ê°œë³„ ë³€ìˆ˜ì˜ ì¶”ì • íšŒê·€ê³„ìˆ˜ë¥¼ í‘œí˜„í•˜ëŠ” plotì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì´ ì—­ì‹œ `statsmodels`ì˜ `sm.graphics.plot_ccpr_grid`ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ê²©ìí˜• CCPR plotì„ ì–»ì„ ìˆ˜ ìˆë‹¤.

```python
# CCPR
fig = plt.figure(figsize=(8,15))
sm.graphics.plot_ccpr_grid(ols_res, fig=fig)
plt.savefig('CCPR.png', transparent = False, facecolor = 'white')
```

![CCPR](/assets/img/ë”°ë¦‰ì´_Linear_4.png){: .align-center}

### Stepwise Selection

Rì—ì„œëŠ” step í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ì „ì§„/í›„ì§„ì„ íƒë²•ì„ ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆìœ¼ë‚˜, ì•ˆíƒ€ê¹ê²Œë„ `statsmodels` íŒ¨í‚¤ì§€ëŠ” ì´ëŸ¬í•œ ëª¨ë“ˆì„ ì œê³µí•˜ê³  ìˆì§€ ì•Šë‹¤. ë”°ë¼ì„œ ì „ì§„ì„ íƒë²•ê³¼ í›„ì§„ì„ íƒë²•ì„ ì§ì ‘ êµ¬í˜„í•˜ëŠ” ì¼ì´ í•„ìš”í•˜ë‹¤. ì½”ë“œëŠ” ì•„ë˜ì™€ ê°™ì´ êµ¬í˜„í–ˆê³ , ì´ë•Œ ë³€ìˆ˜ ì„ íƒ ê¸°ì¤€ì€ AIC<sup>Akaike Information Criterion</sup> í˜¹ì€ BIC, MSEë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œë” ì„¤ì •í–ˆëŠ”ë°([ë‚´ìš©](https://ddangchani.github.io/machine%20learning/Model_Assessment/)) ì´ë“¤ì€ ê°’ì´ ë‚®ì„ìˆ˜ë¡ ë” ì¢‹ì€ ì„±ëŠ¥ì„ì„ ì˜ë¯¸í•˜ë¯€ë¡œ ì´ˆê¸°ê°’ì„ ë¬´í•œ(inf)ìœ¼ë¡œ ì„¤ì •í•´ ê°ì†Œì‹œì¼œë‚˜ê°€ëŠ” ê³µí†µì ì¸ ì•Œê³ ë¦¬ì¦˜ì„ ê°–ê¸° ë•Œë¬¸ì´ë‹¤.

```python
# Forward/Backward-Stepwise Regression
def forward_ols(y, X, score):
    """
    endog y, Exog X
    score = 'aic', 'bic'
    """
    left_cols = list(X.columns.drop('const'))
    selected_cols = ['const']
    current_score, best_new_score = float('inf'), float('inf')
    aics = []
    while left_cols and current_score == best_new_score:
        scores = []
        for col in left_cols:
            sel_i = selected_cols + [col] # ê¸°ì¡´ selectedì— ìƒˆë¡œìš´ ë³€ìˆ˜ í•˜ë‚˜ì”©ë§Œ ì¶”ê°€
            ols_i = sm.OLS(y ,X[sel_i]).fit()
            scores.append(getattr(ols_i, score))
        best_i = np.argmin(scores) # AIC ìµœì†Œë¡œ í•˜ëŠ” ë³€ìˆ˜ index ê°€ì ¸ì˜¤ê¸°
        best_new_score = scores[best_i]
        best_col = left_cols[best_i]
        # best AICì™€ ë¹„êµ
        if current_score > best_new_score:
            left_cols.remove(best_col) # best_col remove
            selected_cols.append(best_col)
            current_score = best_new_score
            aics.append((best_col, current_score)) # AIC ì €ì¥
    model_best = sm.OLS(y, X[selected_cols]).fit()
    return model_best, aics
```

ìœ„ í•¨ìˆ˜ëŠ” Forward-stepwise regressionì„ ì ìš©í•˜ì—¬ ë³€ìˆ˜ë“¤ì„ ì„ íƒí•˜ê³ , ê²°ê³¼ë¡œëŠ” ìµœì  ëª¨ë¸ê³¼ (ê° ë‹¨ê³„ì—ì„œ ì¶”ê°€ëœ ë³€ìˆ˜, ê° ë‹¨ê³„ì—ì„œì˜ score ê°’) ì˜ ìˆœì„œìŒì„ ë„ì¶œí•œë‹¤. ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

```py
ols_f, aics_f = forward_ols(y_train, X_train, 'aic')
print(aics_f)
# Results
# [('temp', 10429.056), ('hour', 10149.413), ('visibility', 10102.013),('precip_1.0', 10077.424), ('windspeed', 10053.549), ('pm10', 10040.123), ('ozone', 10033.958)]
```

ì¦‰, ì´ˆê¸° ìƒìˆ˜í•­(`const`)ë§Œ ìˆëŠ” ëª¨ë¸ë¡œë¶€í„° ì°¨ë¡€ëŒ€ë¡œ `temp`, `hour` ë“±ì„ ì¶”ê°€í•´ë‚˜ê°€ë©° ì ì°¨ AIC ê°’ì„ ê°ì†Œì‹œì¼œë‚˜ê°”ìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ìœ„ `forward_ols` í•¨ìˆ˜ì—ì„œ `score = 'bic'`ë¡œ ì ìš©í•˜ë©´ BIC ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ Forward stepwise algorithmì„ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ”ë°, AICì™€ ë™ì¼í•œ ê²°ê³¼ê°€ ë„ì¶œë¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì´ë ‡ê²Œ ë„ì¶œí•´ë‚¸ variable-selected modelì„ ì´ìš©í•´ ì•ì„  Full-modelì— í–ˆë˜ ê²ƒ ì²˜ëŸ¼ validation dataì— ëŒ€í•´ RMSEë¥¼ êµ¬í•´ë³´ë„ë¡ í•˜ì.

```python
# Validation of variable-selected model
def rmse_val(model, y_val, X_val):
    '''model : fitted model'''
    selected_vars = list(model.params.index)
    X = X_val[selected_vars]
    pred_val = model.predict(X)
    mse = mean_squared_error(y_val, pred_val)
    return np.sqrt(mse).round(3)

rmse_val(ols_aic_f,y_val,X_val) # Result : 53.5
```

RMSEëŠ” 53.242ì˜€ë˜ Full Modelì— ë¹„í•´ ì˜¤íˆë ¤ ì†Œí­ ì¦ê°€í–ˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤ğŸ˜….

ğŸ–¥ Full Code on Github: https://github.com/ddangchani/project_ddareungi

{% endraw %}