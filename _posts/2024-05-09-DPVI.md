---
title: "Differentially Private Variational Inference"
tags: 
- Variational Inference
- Differential Privacy
- Paper Review
use_math: true
---

# Variational Inference with DP

이번 글에서는 다음 두 논문
- Jälkö, J. et al., (2017). _Differentially Private Variational Inference for Non-conjugate Models
- Wang, Y. et al., (2015). Privacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo.
논문 등을 바탕으로 [Variational Inference]({% post_url 2023-12-22-Variational-Inference %}) 프레임워크에 DP<sup>Differential Privacy</sup>가 어떻게 적용될 수 있는지 살펴보고자 한다. 

## Differential Privacy

(DP에 대한 기본적인 정의는 [이 글]({% post_url 2024-01-02-Differential-Privacy %})을 참고)

### Definition and Properties

Randomized algorithm $\mathcal{M}$이 $(\epsilon, \delta)$-DP를 만족한다는 것은 모든 가측집합 $S\subset \mathrm{Range}(\mathcal{M})$ 와 이웃하는 데이터셋 $D,D'$ 에 대해
$$
\Pr(\mathcal{M}(D)\in S) \le e^{\epsilon}\Pr (\mathcal{M}(D')\in S) + \delta \tag{DP}
$$
를 만족하는 것을 말한다. $(\epsilon, \delta)$-DP는 유용한 성질들이 있는데, 대표적인 두 가지는 다음과 같다.

#### Post-processing Immunity
$\mathcal{M}$이 $(\epsilon, \delta)$-DP 인 알고리즘이면, 임의의 함수 $\mathcal{B}$ 에 대해 $\mathcal{B}\circ \mathcal{M}$ 역시 $(\epsilon, \delta)$-DP 이다.

#### Composition rule
$\mathcal{M}_{1}$이 $(\epsilon_{1}, \delta_{1})$-DP 이고, $\mathcal{M}_2$가 $(\epsilon_{2},\delta_{2})$-DP 라면 $\mathcal{M}_{1}\circ \mathcal{M}_{2}$ 는 $(\epsilon_{1}+\epsilon_{2}, \delta_{1}+ \delta_{2})$-DP 를 만족한다.

## Posterior sampling

데이터 $x_{i}\in \mathcal{X}$  들이 주어질 때 모델 $\theta\in \Theta$ (parametric model의 경우는 모수를 의미)의 **사후분포**는 다음과 같이 주어진다.
$$
\pi(\theta\mid X) = \frac{\pi (\theta)\prod_{i}p(x_{i}\mid \theta)}{\int \prod_{i}p(x_{i}\mid \theta)\pi(\theta)d\pi}
$$

### Implicitly Preserving DP

사후분포로부터 모델을 샘플링하는 것을 일종의 randomized algorithm으로 생각할 수 있을 것이다. 그렇다면 이에 대한 $(\epsilon, \delta)$-DP를 고려할 수 있는데, 이에 대해서는 다음과 같은 정리가 성립한다.

1. 로그가능도가 다음을 만족하면 (유계)
	$$
	\sup_{x,\theta}\left\vert \log p(x\mid \theta)\right\vert \le B
	$$
	사후분포 $\pi(\theta\mid x)$ 로부터의 샘플링 $\theta_{1}\sim \pi$ 는 $4B$-DP를 만족한다.

2. $\mathcal{X}$의 정의역이 유계이고 (i.e. $\Vert x\Vert_{\ast}\le R$) 로그가능도가 $L$-lipschitz 라면 샘플링 $\theta_{1}\sim \pi$ 는 $4LR$-DP를 만족한다.

이로부터 다음과 같은 알고리즘을 구성할 수 있다.

![](Pasted%20image%2020240508161802.png)
*OPS algorithm. Wang et al., 2015*





# References
- Jälkö, J., Dikmen, O., & Honkela, A. (2017). _Differentially Private Variational Inference for Non-conjugate Models_ (arXiv:1610.08749). arXiv. [https://doi.org/10.48550/arXiv.1610.08749](https://doi.org/10.48550/arXiv.1610.08749)
- Wang, Y.-X., Fienberg, S., & Smola, A. (2015). Privacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo. _Proceedings of the 32nd International Conference on Machine Learning_, 2493–2502. [https://proceedings.mlr.press/v37/wangg15.html](https://proceedings.mlr.press/v37/wangg15.html)