---
title: "Stochastic Differential Equation"
tags: 
- Deep Learning
- Diffusion
- Generative AI
- SDE
use_math: true
---
# Stochastic Differential Equation

[DDPM]({% post_url 2024-01-25-Diffusion %}) 등의 Diffusion 모델은 확률미분방정식<sup>Stochastic differential equation, SDE</sup>의 형태로부터 구할 수 있다. 비록 SDE 기반의 diffusion 모델은 DDPM 등의 모델보다 뒤늦게 제안되었지만, SDE의 한 종류로 볼 수 있다. 이번 글에서는 diffusion 모델들의 근간이 되는 확률미분방정식 기반의 생성형 모델을 살펴보도록 하겠다.

## ODE and SDE

[이전 글]({% post_url 2024-03-26-NeuralODE %})에서 살펴본 것과 같이, 시간 $t$에서의 확률변수 $X(t)$에 대한 상미분방정식<sup>Ordinary differential equation, ODE</sup>는 다음과 같은 형태로 주어진다.


$$

\frac{dX}{dt}(t) = f(X(t), t) \tag{ODE}


$$

초기값 조건 $X(0)= X_{0}$ 이 주어지면, 위 ODE의 해는 다음과 같다.


$$

X(t) = X_{0}+ \int_{0}^{t}f(X(s),s)ds


$$

확률미분방정식은, ODE에 다음과 같이 *Brownian motion* 항이 추가된 것으로 정의된다.


$$

dX_{t}= f(X_{t},t)dt + g(t)dW_{t} \tag{SDE}


$$

여기서 $W_{t}$는 Brownian motion(혹은 Wiener process)이다. ODE에서와 마찬가지로 다음과 같이 적분 형태의 해를 생각할 수 있다.


$$

X_{t}=X_{0}+ \int_{0}^{t}f(X_{s},s)ds + \int_{0}^{t}g(X_{s},s)dW_{s}


$$

이때, SDE의 해를 위와 같은 적분 형태로 생각하기보다는 확률변수열 $$\{X_{t}\}_{t=0}^{T}$$ 의 결합분포로 생각하는 것이 더 자연스럽다. 또한, 결합분포에서 $t$ 시점의 주변확률분포를 $$X_{t}\sim p_{t}$$ 라고 표현하자.

### Fokker-Planck-Kolmogorov Equation

SDE(식 $(\mathrm{SDE})$) 의 주변확률분포 $X_{t}\sim p_{t}$에 대해 다음과 같은 편미분방정식<sup>partial differential equation</sup>이 주어지는데, 이를 **Fokker-Plank-Kolmogorov(FPK) equation**이라고 한다.


$$

\partial_{t}p_{t}(x) = -\partial_{x}(f(x,t)p_{t}(x)) + \frac{g^{2}(t)}{2}\partial_{x}^{2}p_{t}(x)


$$

다변량 확률분포($d>1$)의 FPK equation은 다음과 같다.


$$

\partial_{t}p_{t}(x) = - \nabla_{x}\cdot(fp_{t}) + \frac{1}{2}{\mathrm{tr}}(g^{\top}\nabla_{x}^{2}p_{t}g)


$$

> 증명 : 부분적분을 이용한 증명 (참고 : Applied Stochastic Differential Equations Ch 5-2

## Reverse-time SDE and Data Generation

Diffusion 모델은 주어진 데이터(e.g. image)를 노이즈로 변환하는 과정을 통해 이미지의 특성을 학습하고, 이러한 노이즈로부터 새로운 데이터를 생성한다. 즉, $X_{0}\sim p_{0}$가 주어진 이미지 데이터의 분포에 대응되며 이로부터 노이즈 분포 $X_{T}\sim p_T$ 를 학습한다. 반면, 새로운 데이터를 생성하기 위해서는 노이즈(e.g. Gaussian)를 랜덤 샘플링하여 $p_T$로 부터 $p_{0}$을 생성해야 한다. 즉, **역방향**(reverse-time) SDE가 필요하고, 이는 다음과 같이 정의된다.

### Theorem (Anderson, 1982)
정방향 확률미분방정식


$$

dX_{t}= f(X_{t},t )dt + g(t)dW_{t},\quad X_{0}\sim p_0


$$

에 대응되는 역방향 확률미분방정식은


$$

d\bar X_{t}=(f(\bar X_{t},t)-g^{2}(t) \nabla_{x}\log p_{t}(\bar X_{t}))dt + g(t)d\bar W_{t}, \bar X_{T}\sim p_{T}\tag{RSDE}


$$

와 같다. 여기서 $\bar W_{t}$는 역방향 Brownian motion을 의미한다.

> 증명 : FPK equation을 사용하여 동치를 보일 수 있다.

## Score Matching

역방향 확률미분방정식 $(\mathrm{RSDE})$를 실제 학습에 적용하기 위해서는 score function $\nabla_x \log p_{t}$에 대한 정보가 필요하다. 그러나 실제로는 분포 $p_{t}$를 직접 구할 수 없기 때문에, 이를 근사하는 신경망 모델을 생성하게 된다. 이를 **Score matching**이라고 하며, 다음과 같은 손실함수를 이용한다.


$$

\mathcal{L}(\theta) = \int_{0}^{T}\lambda(t) \mathrm{E}_{X_{t}}\left[\Vert s_{\theta}(X_{t},t )-\nabla_{X_{t}}\log p_{t}(X_{t})\Vert^{2}\right]dt


$$

이때, 위 손실함수는 아래 두 손실함수와 동치이다.


$$

\begin{align}
\mathcal{L}(\theta) &=  \int_{0}^{T}\lambda(t)\mathrm{E}_{X_{0}}\left[\mathrm{E}_{X_{t}\mid X_{0}}\left[\left\Vert s_\theta(X_{t},t )-\nabla_{X_{t}}\log p_{t\mid 0}(X_{t}\mid X_{0})\right\Vert^{2} \mid X_{0}\right]\right]\\
\mathcal{L}(\theta)&= \int_{0}^{T}\lambda(t)\mathrm{E}_{X_{t}} \left[\Vert s_\theta(X_{t},t)\Vert^{2}+ 2\mathrm{E}_{\nu}\left[\frac{d}{dh}\nu^{\top}s_\theta(X_{t}+h\nu,t)\bigg\vert_{h=0}\right]\right]dt+C
\end{align}


$$

여기서 $\nu\sim\mathcal{N}(0,I)$ 이다. 위 손실함수를 이용하는 방법을 *Denoising score matching(DSM)* 이라고 하며, 아래의 손실함수를 이용하는 방법을 *Sliced score matching(SSM)* 이라고 한다. DSM은 조건부 확률 $X_{t}\mid X_{0}$ 에 대한 분포를 요구하기 때문에, 일반적으로는 SSM이 더 널리 사용된다. 반면 두 방법이 모두 적용가능한 경우에는 DSM이 더 나은 성능을 보인다고 한다.




# References
- 서울대학교 기계지능특강-생성형 인공지능 강의노트
- Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., & Poole, B. (2021). _Score-Based Generative Modeling through Stochastic Differential Equations_ (arXiv:2011.13456). arXiv. [https://doi.org/10.48550/arXiv.2011.13456](https://doi.org/10.48550/arXiv.2011.13456)
-  Murphy, K. P. (2023). _Probabilistic machine learning: Advanced topics_. The MIT Press.
- Särkkä, S., & Solin, A. (2019). _Applied Stochastic Differential Equations_. Cambridge University Press.
