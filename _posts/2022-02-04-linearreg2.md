---
title: "Linear Regression (2)"
tags:
- Linear Model
- Machine Learning
- Principal Component Regression
- Partial Least Squares
category: Linear Model
use_math: true
---
{% raw %}
## Linear Regression - Using Substituted Input
[이전 포스팅](https://ddangchani.github.io/machine learning/linearreg1)에서는 주어진 변수들을 그대로 사용하여 회귀분석하는 다양한 방법을 다루었다. 이번 포스팅에서는 주어진 변수들을 기반으로 새로운 변수들을 만들어 회귀분석을 진행하는 방법을 다루어보도록 한다.   

### 주성분회귀<sup>Principal Components Regression</sup>
주성분(Principal Components)의 본래 의미는 Input Matrix $X$의 고유값과 고유벡터와 관련된다. (자세한 내용은 [Kernel PCA를 설명한 포스팅](https://ddangchani.github.io/machine learning/kernelpca)을 참고)   
m번째 주성분에 해당하는 고유벡터를 $v_m$으로 표기하고, 이에 대응하는 Input Vector를 $\mathbf{z_m=X}_{v_m}$ 라고 정의하자. 이때 $\mathbf{z_m}$ 은 $\mathbf{X}$의 열벡터들의 선형결합으로 나타난다. 따라서, 우리는 종속변수 $\mathbf{Y}$에 대해 $\mathbf{z_1,\ldots,z_m}$ 을 기반으로 다음과 같은 회귀분석을 생각할 수 있을 것이다 ($m<p$).   

$$

\mathbf{\hat{Y}}^{pcr}=\bar{Y}\mathbf{1_n}+\sum_{i=1}^m\hat{\theta}_i\mathbf{z_i}

$$  

이때 각 주성분들은 서로 직교하므로, 회귀계수 $\hat{\theta}_i$는   

$$

\hat{\theta}_i = \frac{\langle\mathbf{z_i,Y\rangle}}{\langle\mathbf{z_i,z_i\rangle}}

$$

으로 구할 수 있다. 또한, 앞서 언급한 것과 같이 각 주성분들은 $X$의 열벡터들의 선형결합으로 표현될 수 있으므로

$$

\hat{\beta}^{pcr}=\sum_{i=1}^m\hat{\theta}_i v_i

$$   

위와 같은 회귀계수의 표현이 가능하다.   

### Partial Least Squares
PLS는 주성분회귀와 비슷하게 주어진 변수들의 선형결합을 바탕으로 회귀모형을 구성한다. 그러나 주성분회귀와는 다르게, PLS에서는 변수들의 선형결합을 설정하는 과정에서 종속변수 $\mathbf{Y}$이용한다.
> **PLS Algorithm**  
> 1. 각 변수 $\mathbf{x_j}$를 표쥰화하고 $\mathbf{\hat{y}}^{(0)}=\bar{y}\mathbf{1}$, $\mathbf{x_j}^{(0)}=\mathbf{x_j}$ 으로 설정한다.
> 2. $m=1,\ldots ,p$ 에 대해   
> a) $\mathbf{z_m}=\sum_{j=1}^p\langle\mathbf{x_j^{(m-1)},y\rangle\mathbf{x_j^{(m-1)}}}$   
> b) $\hat{\theta}_m=\mathbf{\langle z_m,y\rangle/\langle z_m,z_m\rangle}$   
> c) $\mathbf{\hat{y}^{(m)}=\hat{y}^{(m-1)}}+\hat{\theta}_m\mathbf{z_m}$   
> d) $\mathbf{x_j^{(m)}=x_j^{(m-1)}-[\langle z_m,x_j^{m-1)}\rangle/\langle z_m,z_m\rangle]z_m}$ : 각 $\mathbf{x_j^{(m-1)}}$ 을 $\mathbf{z_m}$ 에 대해 직교화함.   
> 3. $m$번째까지의 위 2번의 과정들을 거치면, 식 $\mathbf{\hat{y}^{(m)}=X}\hat{\beta^{PLS}(m)}$ 꼴의 선형 관계식을 얻을 수 있다. 

## 종속변수가 단일변수가 아닌 경우(Multiple Outcome) - CCA
이전 포스팅부터 살펴본 회귀모형의 축소, 변수 선택 문제를 이번에는 종속변수가 다중변수인 경우로 확장해보자. 즉, 종속변수 $Y$가 벡터가 아닌 행렬로 주어지며, 여기서는 $n\times k$ 의 행렬로 주어진다고 가정해보자.   
만일 이전에 살펴본 [릿지 회귀](https://ddangchani.github.io/machine learning/linearreg1)를 이용해 주어진 회귀모형을 규제하려면 어떻게 해야할까? 이 문제는 하이퍼파라미터 $\lambda$를 어떻게 설정하느냐의 문제로 귀결되는데, 결국 한 개의 $\lambda$를 사용하거나 k개의 $\lambda_1,\ldots,\lambda_k$ 를 사용하는 경우로 나누어질 것이다. 이를 일반화해서 생각해보자.   

$$

Y_k = f(X) + \epsilon_k\\
Y_l = f(X) + \epsilon_l

$$   

종속변수 $Y$의 어떤 두 열(column)에 대해 위와 같은 관계식을 만족하도록 모형을 구성하자. 우리가 관심있는 것은 공통된 함수 $f$를 추정하는 것이다. 그러기 위해서는 종속변수의 관측값들을 공통으로 사용할 수 있게끔 pooling 하는 작업이 필요하다. 이와 관련된 기법이 **CCA**(Canonical Correlation Analysis) 이며, Multiple Output 모형을 위해 고안된 차원축소 기법이다.   
CCA는 PCA(Principal component analysis)와 마찬가지로 서로 무관한(직교하는) $$\mathbf{X}$$의 열벡터들의 선형결합 $$\mathbf{X}_{v_{m}}$$ 들과 이에 대응하는 $\mathbf{y_k}$ 들의 선형결합 $$\mathbf{Y}_{u_m}$$ 들을 찾아내는 것이다. 단, 이때 상관계수 $$Cor^2(\mathbf{Y}_{u_m},\mathbf{X}_{v_m})$$ 이 연속적으로(successively) 최대화되어야 한다. 이때 CCA의 해는 표본상관계수행렬 $\mathbf{Y^\top X}/N$ 의 generalized-SVD 를 이용한다.   


# References
 - Elements of Statistical Learning
{% endraw %}