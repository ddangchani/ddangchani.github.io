---
title: "State Space Model"
tags: 
- Deep Learning
- Generative AI
- Mamba
use_math: true
---

[이전]({% post_url 2023-01-12-State-Space_Model %})에 다룬 상태공간모형(State-Space Model, 이하 SSM)은 Markov chain을 기반으로 하는 시계열 모형의 일종으로, 실제 관측가능한 observation 데이터와 hidden state data가 결합하여 만들어집니다. 최근에는 이를 확장하여 State Space Layer(SSL)라는 딥러닝 구조로도 사용되고 있습니다. 특히, **Mamba** (Gu & Dao, 2024)의 등장으로 [Transformer]({% post_url 2024-04-12 %})를 대체할 수 있는 아키텍처의 후보로 여겨지고 있기도 합니다. 이번 글에서는 이러한 State Space Model에 대해 자세히 알아보겠습니다.

# Revisit: State Space Model in Time Series

SSM은 본래 시계열 데이터 분석을 위해 사용되는 모델로, 관측가능한 데이터 $$\mathbf{y}_{t}$$ 와 hidden state data $$\mathbf{x}_{t}$$ 가 결합된 모델입니다. 이를 수식으로 나타내면 다음과 같습니다.

$$
\begin{cases}
\mathbf{x}_{t} = \Phi\mathbf{x}_{t-1}+\gamma \mathbf{u}_{t}+w_{t}\\
\mathbf{y}_{t} = A_{t}\mathbf{x}_{t}+\Gamma\mathbf{u}_{t}+ v_{t}
\end{cases}
$$

이때 noise vector $w_{t}, v_{t}$ 는 각각 정규분포 $N_{p}(0,Q), N_q(0,R)$ 을 따르며 각각 독립이라는 가정이 필요합니다. 이러한 **정규성 가정**(Gaussian assumption)은 상태공간모형에서 매우 중요합니다. 이렇게 주어지는 상태공간모형은 [Kalman Filter]({% post_url 2023-01-12-State-Space_Model %})로 hidden state를 추정할 수 있습니다.

Hidden state를 사용한다는 점에서 RNN

# HiPPO

일반적으로 언어 모델을 만들 때에는 트랜스포머 구조가 많이 사용됩니다. 이전에는 RNN(Recurrent Neural Network)이 많이 사용되었지만, 트랜스포머의 등장으로 RNN은 대체되었습니다. RNN 구조는 계산량 측면에서, 언어 시퀀스를 생성하는데 트랜스포머에 비해 훨씬 더 효율적입니다.




# References

[1]

A. Gu and T. Dao, “Mamba: Linear-Time Sequence Modeling with Selective State Spaces.” arXiv, May 31, 2024. doi: [10.48550/arXiv.2312.00752](https://doi.org/10.48550/arXiv.2312.00752).

[2]

A. Gu, T. Dao, S. Ermon, A. Rudra, and C. Re, “HiPPO: Recurrent Memory with Optimal Polynomial Projections.” arXiv, Oct. 22, 2020. doi: [10.48550/arXiv.2008.07669](https://doi.org/10.48550/arXiv.2008.07669).